{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c7ec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Creating dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8315109f14f848c192387af13d4adc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2631e9c6b74559824eddda2e79d60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'response', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1900\n",
      "})\n",
      "Dataset({\n",
      "    features: ['instruction', 'response', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "MAX_LENGTH = 680\n",
    "\n",
    "# Path to your text file\n",
    "fin = open('data/random_func_array1d_arithmetic_test.json', 'r')\n",
    "\n",
    "lines = fin.readlines()\n",
    "instructions = []\n",
    "responses = []\n",
    "\n",
    "print(\"Loading data ...\")\n",
    "for line in lines:\n",
    "    datum = json.loads(line)\n",
    "    instruction = str(datum[\"results\"]).replace(\"\\'\", '').replace('ZeroDivisionError', 'error').replace(':', '').replace(',', '')\n",
    "    response = datum[\"random_code\"]\n",
    "    for i in range(10):\n",
    "        response = response.replace('  ', ' ')\n",
    "    responses.append(response)\n",
    "    instructions.append(instruction)\n",
    "    if len(responses) % 10000 == 0:\n",
    "        print(len(responses), \"lines loaded\")\n",
    "\n",
    "print(\"Creating dataset ...\")\n",
    "# Create a DataFrame and then a Dataset\n",
    "data = {\"instruction\": instructions, \"response\": responses}\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.05, seed=42)  # 5% for test, 95% for train\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xhyi/PT_GPTNEO350_ATG\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the padding token\n",
    "eos_encoded = tokenizer.encode(tokenizer.eos_token)[0]\n",
    "\n",
    "# Tokenize and process the dataset\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the concatenated instruction and response\n",
    "    tokenized_data = tokenizer([instruction + \" \" + response for instruction, response in zip(examples[\"instruction\"], examples[\"response\"])], truncation=True, add_special_tokens=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "    # Create labels with -100 for the instruction part and token IDs for the response part\n",
    "    labels = []\n",
    "    for i, (instruction, response) in enumerate(zip(examples[\"instruction\"], examples[\"response\"])):\n",
    "        instruction_ids = tokenizer(instruction, add_special_tokens=False)[\"input_ids\"]\n",
    "        response_ids = tokenizer(response, add_special_tokens=False)[\"input_ids\"]\n",
    "        #print(len(instruction_ids) + len(response_ids))\n",
    "        label = [-100] * len(instruction_ids) + response_ids + [eos_encoded] + [-100] * (len(tokenized_data[\"input_ids\"][i]) - len(instruction_ids) - len(response_ids) - 1)\n",
    "        label = label[0:MAX_LENGTH]\n",
    "        labels.append(label)\n",
    "\n",
    "    # Update the tokenized_data with labels\n",
    "    tokenized_data[\"labels\"] = labels\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# The tokenized_dataset now contains input_ids, attention_mask, and labels\n",
    "print(tokenized_train_dataset)\n",
    "print(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105f3ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_train_dataset.save_to_disk('data/random_func_array1d_arithmetic_train_tokenized_v1.dataset')\n",
    "#tokenized_test_dataset.save_to_disk('data/random_func_array1d_arithmetic_test_tokenized_v1.dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d281795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90d1a22b92a4d1b8fc5711f2316ff0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "tokenized_train_dataset = load_from_disk('data/random_func_array1d_arithmetic_train_tokenized_v1.dataset')\n",
    "tokenized_test_dataset = load_from_disk('data/random_func_array1d_arithmetic_test_tokenized_v1.dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b1d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{input [6 7 -25 -1 -3 -3 29 5] output 29} {input [3 -18 -1 -9 8 -3 20] output -3} {input [9 2 -29 9 12] output 9} {input [16 -8 -8 -4 22 -1 -13] output -1} {input [-4 2 -2 -11] output -2} {input [2 9 7 -7 -2 8 -3 31] output -3} {input [-8 -44 15 27 16 -13 -7] output -13} {input [14 -7 -3 1 2 -8 8] output -8} {input [-8 -4 6 2] output 6} {input [8 12] output 8} {input [-15 -35 -7 14 -15] output 14} {input [10 -5 1 -17 -2 -3 1] output -3} {input [-10 0 -3 0 -8 44] output -8} {input [-7 20 6] output 20} {input [9 1 -6 0 -12 -16] output -12} {input [-6 -2 -1 -7 19 12 23 3] output 23} {input [3 26 2 1 7 4 0 3] output 0} {input [-8 5] output -8} {input [-7 -3 9] output -3} {input [3 -21 -34 22] output -34} {input [-6 15 7] output 15} {input [14 -20 -13 19 0 4 5 11] output 5} {input [-13 -2 1 11 8 2 -2 -7] output -2} {input [4 -3 -16 2 -1 2] output -1} {input [1 21 -16 4] output -16} {input [0 -14 2] output -14} {input [3 21 9 8 -6 -15 -3] output -15} {input [-8 0 13 -3 -2 3] output -2} {input [1 -10 -4] output -10} {input [-3 31 -3 -8 2 -9 -12 24] output -12} {input [-11 8 1 1 0 6 6] output 6} {input [-10 -3 -3 17 10] output 17} {input [3 -17] output 3} {input [-2 -15 16 -1 -3 -4 -27] output -4} {input [26 36 -3] output 36} {input [-24 -6 -8 -15 5] output -15} {input [11 11 3 -5 1 -14] output 1} {input [11 -5 -7] output -5} {input [6 19 -22 11 -1] output 11} {input [-5 7 6 17 12 -16 11] output -16}] def foo(arr):\n",
      " if len(arr) == 1:\n",
      " return arr[0]\n",
      " else:\n",
      " if len(arr) == 2:\n",
      " return arr[0]\n",
      " else:\n",
      " return foo(arr[1:])\n",
      " <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "680\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&def foo(arr):\n",
      " if len(arr) == 1:\n",
      " return arr[0]\n",
      " else:\n",
      " if len(arr) == 2:\n",
      " return arr[0]\n",
      " else:\n",
      " return foo(arr[1:])\n",
      " <|endoftext|>&&&&&&&&&&&&&&&&&&&&\n"
     ]
    }
   ],
   "source": [
    "i=5\n",
    "print(tokenizer.decode(tokenized_test_dataset[i]['input_ids']))\n",
    "print(len(tokenized_test_dataset[i]['labels']))\n",
    "print(tokenizer.decode([max(i,x) for x in tokenized_test_dataset[i]['labels']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bead2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [\n",
      "1 {\n",
      "2 input\n",
      "3  [\n",
      "4 1\n",
      "5  6\n",
      "6  -\n",
      "7 2\n",
      "8  1\n",
      "9  8\n",
      "10 ]\n",
      "11  output\n",
      "12  1\n",
      "13 }\n",
      "14  {\n",
      "15 input\n",
      "16  [\n",
      "17 0\n",
      "18  3\n",
      "19  -\n",
      "20 10\n",
      "21 ]\n",
      "22  output\n",
      "23  0\n",
      "24 }\n",
      "25  {\n",
      "26 input\n",
      "27  [\n",
      "28 21\n",
      "29  -\n",
      "30 17\n",
      "31  1\n",
      "32 ]\n",
      "33  output\n",
      "34  21\n",
      "35 }\n",
      "36  {\n",
      "37 input\n",
      "38  [\n",
      "39 4\n",
      "40  -\n",
      "41 1\n",
      "42 ]\n",
      "43  output\n",
      "44  4\n",
      "45 }\n",
      "46  {\n",
      "47 input\n",
      "48  [-\n",
      "49 7\n",
      "50  -\n",
      "51 9\n",
      "52  -\n",
      "53 4\n",
      "54  -\n",
      "55 2\n",
      "56  3\n",
      "57 ]\n",
      "58  output\n",
      "59  -\n",
      "60 7\n",
      "61 }\n",
      "62  {\n",
      "63 input\n",
      "64  [-\n",
      "65 9\n",
      "66  2\n",
      "67  -\n",
      "68 6\n",
      "69  -\n",
      "70 15\n",
      "71  14\n",
      "72  10\n",
      "73  -\n",
      "74 1\n",
      "75 ]\n",
      "76  output\n",
      "77  -\n",
      "78 9\n",
      "79 }\n",
      "80  {\n",
      "81 input\n",
      "82  [-\n",
      "83 10\n",
      "84  4\n",
      "85  4\n",
      "86  -\n",
      "87 1\n",
      "88  17\n",
      "89  11\n",
      "90 ]\n",
      "91  output\n",
      "92  -\n",
      "93 10\n",
      "94 }\n",
      "95  {\n",
      "96 input\n",
      "97  [-\n",
      "98 6\n",
      "99  -\n",
      "100 32\n",
      "101  -\n",
      "102 5\n",
      "103  6\n",
      "104 ]\n",
      "105  output\n",
      "106  -\n",
      "107 6\n",
      "108 }\n",
      "109  {\n",
      "110 input\n",
      "111  [-\n",
      "112 4\n",
      "113  -\n",
      "114 8\n",
      "115  8\n",
      "116 ]\n",
      "117  output\n",
      "118  -\n",
      "119 4\n",
      "120 }\n",
      "121  {\n",
      "122 input\n",
      "123  [\n",
      "124 16\n",
      "125  -\n",
      "126 2\n",
      "127  3\n",
      "128  0\n",
      "129 ]\n",
      "130  output\n",
      "131  16\n",
      "132 }\n",
      "133  {\n",
      "134 input\n",
      "135  [\n",
      "136 6\n",
      "137  -\n",
      "138 2\n",
      "139  1\n",
      "140  -\n",
      "141 4\n",
      "142  2\n",
      "143 ]\n",
      "144  output\n",
      "145  6\n",
      "146 }\n",
      "147  {\n",
      "148 input\n",
      "149  [-\n",
      "150 16\n",
      "151  7\n",
      "152  -\n",
      "153 6\n",
      "154 ]\n",
      "155  output\n",
      "156  -\n",
      "157 16\n",
      "158 }\n",
      "159  {\n",
      "160 input\n",
      "161  [\n",
      "162 1\n",
      "163  -\n",
      "164 7\n",
      "165  -\n",
      "166 3\n",
      "167  -\n",
      "168 22\n",
      "169  0\n",
      "170 ]\n",
      "171  output\n",
      "172  1\n",
      "173 }\n",
      "174  {\n",
      "175 input\n",
      "176  [\n",
      "177 3\n",
      "178  -\n",
      "179 10\n",
      "180  0\n",
      "181  2\n",
      "182 ]\n",
      "183  output\n",
      "184  3\n",
      "185 }\n",
      "186  {\n",
      "187 input\n",
      "188  [\n",
      "189 5\n",
      "190  0\n",
      "191  -\n",
      "192 27\n",
      "193  4\n",
      "194  26\n",
      "195  0\n",
      "196  9\n",
      "197  0\n",
      "198 ]\n",
      "199  output\n",
      "200  5\n",
      "201 }\n",
      "202  {\n",
      "203 input\n",
      "204  [\n",
      "205 11\n",
      "206  8\n",
      "207  4\n",
      "208  0\n",
      "209  2\n",
      "210 ]\n",
      "211  output\n",
      "212  11\n",
      "213 }\n",
      "214  {\n",
      "215 input\n",
      "216  [\n",
      "217 2\n",
      "218  -\n",
      "219 6\n",
      "220  -\n",
      "221 9\n",
      "222 ]\n",
      "223  output\n",
      "224  2\n",
      "225 }\n",
      "226  {\n",
      "227 input\n",
      "228  [\n",
      "229 10\n",
      "230  2\n",
      "231  23\n",
      "232  1\n",
      "233 ]\n",
      "234  output\n",
      "235  10\n",
      "236 }\n",
      "237  {\n",
      "238 input\n",
      "239  [-\n",
      "240 8\n",
      "241  -\n",
      "242 7\n",
      "243  5\n",
      "244  -\n",
      "245 2\n",
      "246  1\n",
      "247 ]\n",
      "248  output\n",
      "249  -\n",
      "250 8\n",
      "251 }\n",
      "252  {\n",
      "253 input\n",
      "254  [-\n",
      "255 1\n",
      "256  4\n",
      "257  9\n",
      "258  43\n",
      "259  4\n",
      "260  -\n",
      "261 10\n",
      "262  -\n",
      "263 7\n",
      "264  2\n",
      "265 ]\n",
      "266  output\n",
      "267  -\n",
      "268 1\n",
      "269 }\n",
      "270  {\n",
      "271 input\n",
      "272  [-\n",
      "273 5\n",
      "274  11\n",
      "275 ]\n",
      "276  output\n",
      "277  -\n",
      "278 5\n",
      "279 }\n",
      "280  {\n",
      "281 input\n",
      "282  [\n",
      "283 0\n",
      "284  4\n",
      "285  -\n",
      "286 3\n",
      "287  -\n",
      "288 2\n",
      "289  -\n",
      "290 5\n",
      "291  -\n",
      "292 6\n",
      "293  -\n",
      "294 6\n",
      "295  3\n",
      "296 ]\n",
      "297  output\n",
      "298  0\n",
      "299 }\n",
      "300  {\n",
      "301 input\n",
      "302  [\n",
      "303 6\n",
      "304  6\n",
      "305  -\n",
      "306 1\n",
      "307 ]\n",
      "308  output\n",
      "309  6\n",
      "310 }\n",
      "311  {\n",
      "312 input\n",
      "313  [\n",
      "314 12\n",
      "315  -\n",
      "316 10\n",
      "317  2\n",
      "318  5\n",
      "319  4\n",
      "320  0\n",
      "321  11\n",
      "322  16\n",
      "323 ]\n",
      "324  output\n",
      "325  12\n",
      "326 }\n",
      "327  {\n",
      "328 input\n",
      "329  [-\n",
      "330 18\n",
      "331  31\n",
      "332 ]\n",
      "333  output\n",
      "334  -\n",
      "335 18\n",
      "336 }\n",
      "337  {\n",
      "338 input\n",
      "339  [\n",
      "340 35\n",
      "341  2\n",
      "342  5\n",
      "343  11\n",
      "344  7\n",
      "345  3\n",
      "346  -\n",
      "347 20\n",
      "348 ]\n",
      "349  output\n",
      "350  35\n",
      "351 }\n",
      "352  {\n",
      "353 input\n",
      "354  [-\n",
      "355 11\n",
      "356  7\n",
      "357  11\n",
      "358 ]\n",
      "359  output\n",
      "360  -\n",
      "361 11\n",
      "362 }\n",
      "363  {\n",
      "364 input\n",
      "365  [\n",
      "366 0\n",
      "367  -\n",
      "368 9\n",
      "369  26\n",
      "370  2\n",
      "371  0\n",
      "372  -\n",
      "373 5\n",
      "374  -\n",
      "375 4\n",
      "376 ]\n",
      "377  output\n",
      "378  0\n",
      "379 }\n",
      "380  {\n",
      "381 input\n",
      "382  [-\n",
      "383 5\n",
      "384  25\n",
      "385 ]\n",
      "386  output\n",
      "387  -\n",
      "388 5\n",
      "389 }\n",
      "390  {\n",
      "391 input\n",
      "392  [-\n",
      "393 8\n",
      "394  11\n",
      "395  11\n",
      "396  0\n",
      "397  -\n",
      "398 10\n",
      "399  -\n",
      "400 13\n",
      "401  -\n",
      "402 16\n",
      "403  3\n",
      "404 ]\n",
      "405  output\n",
      "406  -\n",
      "407 8\n",
      "408 }\n",
      "409  {\n",
      "410 input\n",
      "411  [\n",
      "412 3\n",
      "413  -\n",
      "414 9\n",
      "415  2\n",
      "416  3\n",
      "417  4\n",
      "418 ]\n",
      "419  output\n",
      "420  3\n",
      "421 }\n",
      "422  {\n",
      "423 input\n",
      "424  [-\n",
      "425 6\n",
      "426  -\n",
      "427 3\n",
      "428  18\n",
      "429  15\n",
      "430  -\n",
      "431 31\n",
      "432  -\n",
      "433 9\n",
      "434  3\n",
      "435  0\n",
      "436 ]\n",
      "437  output\n",
      "438  -\n",
      "439 6\n",
      "440 }\n",
      "441  {\n",
      "442 input\n",
      "443  [\n",
      "444 9\n",
      "445  -\n",
      "446 1\n",
      "447  8\n",
      "448  0\n",
      "449 ]\n",
      "450  output\n",
      "451  9\n",
      "452 }\n",
      "453  {\n",
      "454 input\n",
      "455  [-\n",
      "456 24\n",
      "457  0\n",
      "458  0\n",
      "459  9\n",
      "460  9\n",
      "461  -\n",
      "462 34\n",
      "463  3\n",
      "464 ]\n",
      "465  output\n",
      "466  -\n",
      "467 24\n",
      "468 }\n",
      "469  {\n",
      "470 input\n",
      "471  [\n",
      "472 2\n",
      "473  -\n",
      "474 4\n",
      "475  -\n",
      "476 4\n",
      "477 ]\n",
      "478  output\n",
      "479  2\n",
      "480 }\n",
      "481  {\n",
      "482 input\n",
      "483  [-\n",
      "484 10\n",
      "485  -\n",
      "486 4\n",
      "487  -\n",
      "488 1\n",
      "489 ]\n",
      "490  output\n",
      "491  -\n",
      "492 10\n",
      "493 }\n",
      "494  {\n",
      "495 input\n",
      "496  [-\n",
      "497 8\n",
      "498  4\n",
      "499  19\n",
      "500  2\n",
      "501  11\n",
      "502  4\n",
      "503  -\n",
      "504 17\n",
      "505  -\n",
      "506 12\n",
      "507 ]\n",
      "508  output\n",
      "509  -\n",
      "510 8\n",
      "511 }\n",
      "512  {\n",
      "513 input\n",
      "514  [-\n",
      "515 2\n",
      "516  3\n",
      "517  -\n",
      "518 10\n",
      "519  -\n",
      "520 3\n",
      "521  -\n",
      "522 13\n",
      "523  16\n",
      "524 ]\n",
      "525  output\n",
      "526  -\n",
      "527 2\n",
      "528 }\n",
      "529  {\n",
      "530 input\n",
      "531  [-\n",
      "532 8\n",
      "533  0\n",
      "534  2\n",
      "535  -\n",
      "536 34\n",
      "537  -\n",
      "538 15\n",
      "539  -\n",
      "540 3\n",
      "541 ]\n",
      "542  output\n",
      "543  -\n",
      "544 8\n",
      "545 }\n",
      "546  {\n",
      "547 input\n",
      "548  [\n",
      "549 17\n",
      "550  6\n",
      "551  15\n",
      "552  -\n",
      "553 13\n",
      "554  -\n",
      "555 5\n",
      "556  -\n",
      "557 16\n",
      "558  -\n",
      "559 5\n",
      "560  12\n",
      "561 ]\n",
      "562  output\n",
      "563  17\n",
      "564 }\n",
      "565 ]\n",
      "566  def\n",
      "567  foo\n",
      "568 (\n",
      "569 arr\n",
      "570 ):\n",
      "571 \n",
      "\n",
      "572  if\n",
      "573  len\n",
      "574 (\n",
      "575 arr\n",
      "576 )\n",
      "577  ==\n",
      "578  1\n",
      "579 :\n",
      "580 \n",
      "\n",
      "581  return\n",
      "582  arr\n",
      "583 [\n",
      "584 0\n",
      "585 ]\n",
      "586 \n",
      "\n",
      "587  else\n",
      "588 :\n",
      "589 \n",
      "\n",
      "590  return\n",
      "591  arr\n",
      "592 [\n",
      "593 0\n",
      "594 ]\n",
      "595 \n",
      "\n",
      "596  \n",
      "597 <|endoftext|>\n",
      "598 <|endoftext|>\n",
      "599 <|endoftext|>\n",
      "600 <|endoftext|>\n",
      "601 <|endoftext|>\n",
      "602 <|endoftext|>\n",
      "603 <|endoftext|>\n",
      "604 <|endoftext|>\n",
      "605 <|endoftext|>\n",
      "606 <|endoftext|>\n",
      "607 <|endoftext|>\n",
      "608 <|endoftext|>\n",
      "609 <|endoftext|>\n",
      "610 <|endoftext|>\n",
      "611 <|endoftext|>\n",
      "612 <|endoftext|>\n",
      "613 <|endoftext|>\n",
      "614 <|endoftext|>\n",
      "615 <|endoftext|>\n",
      "616 <|endoftext|>\n",
      "617 <|endoftext|>\n",
      "618 <|endoftext|>\n",
      "619 <|endoftext|>\n",
      "620 <|endoftext|>\n",
      "621 <|endoftext|>\n",
      "622 <|endoftext|>\n",
      "623 <|endoftext|>\n",
      "624 <|endoftext|>\n",
      "625 <|endoftext|>\n",
      "626 <|endoftext|>\n",
      "627 <|endoftext|>\n",
      "628 <|endoftext|>\n",
      "629 <|endoftext|>\n",
      "630 <|endoftext|>\n",
      "631 <|endoftext|>\n",
      "632 <|endoftext|>\n",
      "633 <|endoftext|>\n",
      "634 <|endoftext|>\n",
      "635 <|endoftext|>\n",
      "636 <|endoftext|>\n",
      "637 <|endoftext|>\n",
      "638 <|endoftext|>\n",
      "639 <|endoftext|>\n",
      "640 <|endoftext|>\n",
      "641 <|endoftext|>\n",
      "642 <|endoftext|>\n",
      "643 <|endoftext|>\n",
      "644 <|endoftext|>\n",
      "645 <|endoftext|>\n",
      "646 <|endoftext|>\n",
      "647 <|endoftext|>\n",
      "648 <|endoftext|>\n",
      "649 <|endoftext|>\n",
      "650 <|endoftext|>\n",
      "651 <|endoftext|>\n",
      "652 <|endoftext|>\n",
      "653 <|endoftext|>\n",
      "654 <|endoftext|>\n",
      "655 <|endoftext|>\n",
      "656 <|endoftext|>\n",
      "657 <|endoftext|>\n",
      "658 <|endoftext|>\n",
      "659 <|endoftext|>\n",
      "660 <|endoftext|>\n",
      "661 <|endoftext|>\n",
      "662 <|endoftext|>\n",
      "663 <|endoftext|>\n",
      "664 <|endoftext|>\n",
      "665 <|endoftext|>\n",
      "666 <|endoftext|>\n",
      "667 <|endoftext|>\n",
      "668 <|endoftext|>\n",
      "669 <|endoftext|>\n",
      "670 <|endoftext|>\n",
      "671 <|endoftext|>\n",
      "672 <|endoftext|>\n",
      "673 <|endoftext|>\n",
      "674 <|endoftext|>\n",
      "675 <|endoftext|>\n",
      "676 <|endoftext|>\n",
      "677 <|endoftext|>\n",
      "678 <|endoftext|>\n",
      "679 <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized_test_dataset[0]['input_ids']\n",
    "for i in range(MAX_LENGTH):\n",
    "    print(i, tokenizer.decode([input_ids[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541eea41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from transformers import AutoModel\n",
    "\n",
    "def initialize_weights(model, init_type='xavier'):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if init_type == 'xavier':\n",
    "                init.xavier_uniform_(module.weight)\n",
    "            elif init_type == 'he':\n",
    "                init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"xhyi/PT_GPTNEO350_ATG\") #\"EleutherAI/gpt-neo-1.3B\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"xhyi/PT_GPTNEO350_ATG\") #\"EleutherAI/gpt-neo-1.3B\") #\n",
    "\n",
    "# Initialize weights\n",
    "initialize_weights(model, init_type='he')  # Or 'he' for He initialization\n",
    "\n",
    "\n",
    "# # Tokenize and preprocess the dataset\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "#     inputs[\"labels\"] = inputs.input_ids.copy()\n",
    "#     return inputs\n",
    "\n",
    "# tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "# tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a6aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxiaoxinyin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mcwave/code/automath/autocode/wandb/run-20240331_164434-zhkvya1s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xiaoxinyin/huggingface/runs/zhkvya1s' target=\"_blank\">unique-morning-265</a></strong> to <a href='https://wandb.ai/xiaoxinyin/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xiaoxinyin/huggingface' target=\"_blank\">https://wandb.ai/xiaoxinyin/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xiaoxinyin/huggingface/runs/zhkvya1s' target=\"_blank\">https://wandb.ai/xiaoxinyin/huggingface/runs/zhkvya1s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='326892' max='791670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [326892/791670 90:13:19 < 128:16:45, 1.01 it/s, Epoch 2.06/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.097327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>0.083678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.074212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.069186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.063345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.060568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.058540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.057395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.055768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.054824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.053568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.052617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.052288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.051656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300000</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.050672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320000</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.050538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/results-gptneo350m\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=20000,\n",
    "    eval_steps=20000,\n",
    "    logging_steps=20000,\n",
    "    logging_dir=\"./data/logs\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model, 'data/results/gptneo350m-random_func_array1d_arithmetic-v1-417600.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74617ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('data/results/gptneo350m-random_func_array1d_arithmetic-v1-417600.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenized_test_dataset[0]\n",
    "label = example['labels']\n",
    "input_ids = list(example['input_ids'])\n",
    "\n",
    "i = 0\n",
    "while label[i] == -100:\n",
    "    i += 1\n",
    "instruction_len = i\n",
    "\n",
    "for i in range(instruction_len, len(input_ids)):\n",
    "    input_ids[i] = 50256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18607ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "example = tokenized_test_dataset[30]\n",
    "label = example['labels']\n",
    "input_ids = list(example['input_ids'])\n",
    "\n",
    "i = 0\n",
    "while label[i] == -100:\n",
    "    i += 1\n",
    "instruction_len = i + 5\n",
    "\n",
    "for i in range(instruction_len, len(input_ids)):\n",
    "    input_ids[i] = 50256\n",
    "\n",
    "print(example['instruction'])\n",
    "print(example['response'])\n",
    "\n",
    "START_IDX = instruction_len\n",
    "\n",
    "inputs_ids = torch.tensor(input_ids)\n",
    "attention_mask = torch.ones(MAX_LENGTH)\n",
    "idx = START_IDX\n",
    "\n",
    "while idx < len(input_ids):\n",
    "    #print(\"input_ids:\", tokenizer.decode(inputs_ids))\n",
    "    #print(\"inputs_embeds:\", example['inputs_embeds'][758:770])\n",
    "    #print(\"attention_mask:\", example['attention_mask'][768:780])\n",
    "    #print(\"labels:\", tokenizer.decode(example['labels'][768:780]))\n",
    "    outputs = model(input_ids=torch.unsqueeze(inputs_ids, 0).to('cuda:0'),\n",
    "                    attention_mask=torch.unsqueeze(attention_mask, 0).to('cuda:0'))\n",
    "    #\n",
    "    logits = outputs.logits\n",
    "    #print(\"logits:\", logits[:, idx, :])\n",
    "    predicted_token_id = torch.argmax(logits[:, idx, :], dim=-1)\n",
    "    if predicted_token_id == eos_encoded:\n",
    "        break\n",
    "    # Convert the token ID to the actual token\n",
    "    predicted_token = tokenizer.decode(predicted_token_id)\n",
    "    print(idx, predicted_token)\n",
    "    idx += 1\n",
    "    #if idx > START_IDX+2:\n",
    "    inputs_ids[idx] = predicted_token_id.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing \n",
    "\n",
    "a = [1,2,-3,4,5]\n",
    "\n",
    "def squared_sum(arr):\n",
    "    return sum([x*x for x in arr])\n",
    "\n",
    "def abs_sum(arr):\n",
    "    return sum([abs(x) for x in arr])\n",
    "\n",
    "def sum2(arr):\n",
    "    return 2*sum(arr)\n",
    "\n",
    "def sum5(arr):\n",
    "    return 5*sum(arr)\n",
    "\n",
    "def sum_plus_len(arr):\n",
    "    return sum(arr) + len(arr)\n",
    "\n",
    "def sum_plus_2len(arr):\n",
    "    return sum(arr) + 2*len(arr)\n",
    "\n",
    "def sum_minus_len(arr):\n",
    "    return sum(arr) - len(arr)\n",
    "\n",
    "def sum_positive(arr):\n",
    "    return sum([x for x in arr if x > 0])\n",
    "\n",
    "def sum_negative(arr):\n",
    "    return sum([x for x in arr if x < 0])\n",
    "\n",
    "def sum_greater_than_3(arr):\n",
    "    return sum([x for x in arr if x > 3])\n",
    "\n",
    "def sum_mod_3(arr):\n",
    "    return sum([x%3 for x in arr])\n",
    "\n",
    "def interleaving_substract(arr):\n",
    "    if len(arr) == 1:\n",
    "        return arr[0]\n",
    "    else:\n",
    "        return arr[0] - interleaving_substract(arr[1:])\n",
    "    \n",
    "def product(arr):\n",
    "    if len(arr) == 1:\n",
    "        return arr[0]\n",
    "    else:\n",
    "        return arr[0] * product(arr[1:])\n",
    "    \n",
    "def product_plus1(arr):\n",
    "    if len(arr) == 1:\n",
    "        return arr[0] + 1\n",
    "    else:\n",
    "        return (arr[0] + 1) * product(arr[1:])\n",
    "    \n",
    "def product_plus2(arr):\n",
    "    if len(arr) == 1:\n",
    "        return arr[0] + 2\n",
    "    else:\n",
    "        return (arr[0] + 2) * product(arr[1:])\n",
    "    \n",
    "def first(arr):\n",
    "    return arr[0]\n",
    "\n",
    "def last(arr):\n",
    "    return arr[-1]\n",
    "\n",
    "def second(arr):\n",
    "    return arr[1]\n",
    "\n",
    "def third(arr):\n",
    "    return arr[2]\n",
    "\n",
    "def second2last(arr):\n",
    "    return arr[-2]\n",
    "\n",
    "def third2last(arr):\n",
    "    return arr[-3]\n",
    "    \n",
    "test_functions = [sum, max, min, squared_sum, abs_sum, sum2, sum5, sum_plus_len, sum_plus_2len, \n",
    "                  sum_minus_len, sum_positive, sum_negative, sum_greater_than_3, sum_mod_3, \n",
    "                  interleaving_substract, product, product_plus1, product_plus2, first, last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def format_code(code):\n",
    "    INDENT = \"    \"\n",
    "    lines = [l.strip() for l in code.split('\\n')]\n",
    "    lines = [l for l in lines if len(l) > 2]\n",
    "    formatted = lines[0] + '\\n'\n",
    "    current_indent = \"\"\n",
    "    for i in range(1, len(lines)):\n",
    "        if lines[i-1].endswith(':'):\n",
    "            current_indent += INDENT\n",
    "        else:\n",
    "            current_indent = current_indent[0:-len(INDENT)]\n",
    "        formatted += current_indent + lines[i] + '\\n'\n",
    "    return formatted\n",
    "\n",
    "def generate_random_array():\n",
    "    length = random.randint(2, 8)  # Randomly choose the length of the array\n",
    "    # Generate the array with absolute values following an exponential distribution\n",
    "    array = [(-1)**random.randint(0, 1) * round(np.random.exponential(scale=8.0)) for _ in range(length)]\n",
    "    return array\n",
    "\n",
    "def create_test_input(foo, num_random_array=40):\n",
    "    datum = {}\n",
    "    #\n",
    "    datum['random_code'] = \"def foo(arr):\\n\"\n",
    "    results = []\n",
    "    random_arrays = []\n",
    "    random_results = []\n",
    "    for j in range(num_random_array):\n",
    "        random_array = generate_random_array()\n",
    "        random_arrays.append(random_array)\n",
    "        try:\n",
    "            result = foo(random_array)\n",
    "        except Exception as e:\n",
    "            result = type(e).__name__\n",
    "        random_results.append(result)\n",
    "        results.append({\"input\":str(random_array), \"output\":result})\n",
    "    datum['results'] = results\n",
    "    instruction = str(datum[\"results\"]).replace(\"\\'\", '').replace('ZeroDivisionError', 'error').replace(':', '').replace(',', '')\n",
    "    response = datum[\"random_code\"]\n",
    "    for i in range(10):\n",
    "        response = response.replace('  ', ' ')\n",
    "    return instruction, response, random_arrays, random_results\n",
    "\n",
    "def get_input_ids(foo, num_random_array=40):\n",
    "    instruction, response, random_arrays, random_results = create_test_input(foo, num_random_array=num_random_array)\n",
    "    #print(instruction)\n",
    "    tokenized_data = tokenizer(instruction + \" \" + response, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "    return tokenized_data, random_arrays, random_results\n",
    "\n",
    "def check_infer_function(f, verbose=False):\n",
    "    print(\"\\nFUNCTION:\", f.__name__)\n",
    "    tokenized_data, random_arrays, random_results = get_input_ids(f)\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    #print(input_ids)\n",
    "    #tokenizer.decode(input_ids)\n",
    "    instruction_len = len([x for x in input_ids if x < 50256])\n",
    "    START_IDX = instruction_len\n",
    "    inputs_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.ones(MAX_LENGTH)\n",
    "    idx = START_IDX\n",
    "    #\n",
    "    try:\n",
    "        output = \"def foo(arr):\\n\"\n",
    "        while idx < len(input_ids):\n",
    "            #print(\"input_ids:\", tokenizer.decode(inputs_ids))\n",
    "            #print(\"inputs_embeds:\", example['inputs_embeds'][758:770])\n",
    "            #print(\"attention_mask:\", example['attention_mask'][768:780])\n",
    "            #print(\"labels:\", tokenizer.decode(example['labels'][768:780]))\n",
    "            outputs = model(input_ids=torch.unsqueeze(inputs_ids, 0).to('cuda:0'),\n",
    "                            attention_mask=torch.unsqueeze(attention_mask, 0).to('cuda:0'))\n",
    "            #\n",
    "            logits = outputs.logits\n",
    "            #print(\"logits:\", logits[:, idx, :])\n",
    "            predicted_token_id = torch.argmax(logits[:, idx, :], dim=-1)\n",
    "            if predicted_token_id == eos_encoded:\n",
    "                break\n",
    "            # Convert the token ID to the actual token\n",
    "            predicted_token = tokenizer.decode(predicted_token_id)\n",
    "            output += predicted_token\n",
    "            #print(idx, predicted_token)\n",
    "            idx += 1\n",
    "            inputs_ids[idx] = predicted_token_id.detach().cpu().numpy()[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "    #\n",
    "    inferred_code = format_code(output)\n",
    "    print(inferred_code)\n",
    "    exec(inferred_code, globals())\n",
    "    #\n",
    "    count_equal = 0\n",
    "    for i in range(len(random_arrays)):\n",
    "        try:\n",
    "            result = foo(random_arrays[i])\n",
    "        except Exception as e:\n",
    "            result = type(e).__name__\n",
    "        if verbose:\n",
    "            print(\"Random array:\", random_arrays[i])\n",
    "            print(\"Expected result:\", random_results[i])\n",
    "            print(\"Actual result:  \", result)\n",
    "        if random_results[i] == result:\n",
    "            count_equal += 1\n",
    "    #\n",
    "    print(count_equal, '/', len(random_arrays))\n",
    "\n",
    "for f in test_functions:\n",
    "    check_infer_function(f)\n",
    "\n",
    "#check_infer_function(min, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ebdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def foo(arr):\n",
    "#         if len(arr) == 1:\n",
    "#             return arr[0]\n",
    "#         else:\n",
    "#             return max([arr[0], foo(arr[1:])])\n",
    "\n",
    "del foo\n",
    "foo([8, 11, 13, -5, 6, -13, -8, 2, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f309c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(inferred_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200",
   "language": "python",
   "name": "torch200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
