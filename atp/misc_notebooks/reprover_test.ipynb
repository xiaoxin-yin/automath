{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375ede99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rw [<a>Nat.gcd_comm</a>]\n",
      "\n",
      "rw [<a>Nat.gcd_comm</a>]\n",
      "rw [<a>Nat.gcd_comm</a>, <a>Nat.gcd_1</a>]\n",
      "rw [<a>Nat.gcd_comm</a>, <a>Nat.gcd_rec</a>]\n",
      "rw [<a>Nat.gcd_comm</a>, <a>Nat.gcd_gcd_self_left_right</a>]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kaiyuy/leandojo-lean4-tacgen-byt5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"kaiyuy/leandojo-lean4-tacgen-byt5-small\")\n",
    "\n",
    "state = \"n : ℕ\\n⊢ gcd n n = n\"\n",
    "tokenized_state = tokenizer(state, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a single tactic.\n",
    "tactic_ids = model.generate(tokenized_state.input_ids, max_length=1024)\n",
    "tactic = tokenizer.decode(tactic_ids[0], skip_special_tokens=True)\n",
    "print(tactic, end=\"\\n\\n\")\n",
    "\n",
    "# Generate multiple tactics via beam search.\n",
    "tactic_candidates_ids = model.generate(\n",
    "    tokenized_state.input_ids,\n",
    "    max_length=1024,\n",
    "    num_beams=4,\n",
    "    length_penalty=0.0,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=4,\n",
    "    early_stopping=False,\n",
    ")\n",
    "tactic_candidates = tokenizer.batch_decode(\n",
    "    tactic_candidates_ids, skip_special_tokens=True\n",
    ")\n",
    "for tac in tactic_candidates:\n",
    "    print(tac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a123b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ INPUT ------\n",
      " n : ℕ\n",
      "⊢ gcd n n = n\n",
      "\n",
      "------ OUTPUT ------\n",
      "rw [<a>gcd_eq_nth_of_gcd_eq_one</a> n n (n + 1) = n : ℕ : ℕ → ℕ → <a>GCD</gcd</n n gcd_n n = n : ℕ : ℕ → ℕ → <a>GCD</gcd</n n = n := <a>EqGCD.gcd_eq_nth_of_n n (n + 1) n <;> simp [<a>gcd_eqgcd_n_neg_nth_eq_n n]\n",
      "\n",
      "rw [<a>gcd_eq_nth_of_gcd_eq_one</a>, <a>gcd_eq_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth\n",
      "rw [<a>gcd_eq_nth_of_gcd_eq_one</a>, <a>gcd_eq_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_gcd n \n",
      "rw [<a>gcd_eq_nth_of_gcd_eq_one</a>, <a>gcd_eq_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_ord gc\n",
      "rw [<a>gcd_eq_nth_of_gcd_eq_one</a>, <a>gcd_eq_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_nth_of_n h\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kaiyuy/leandojo-lean4-retriever-tacgen-byt5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"kaiyuy/leandojo-lean4-retriever-tacgen-byt5-small\")\n",
    "\n",
    "state = \"n : ℕ\\n⊢ gcd n n = n\"\n",
    "retrieved_premises = [\n",
    "  \"def <a>Nat.gcd</a> : Nat → Nat → Nat\\n| 0        y := y\\n| (succ x) y := have y % succ x < succ x, from mod_lt _ $ succ_pos _,\\n                gcd (y % succ x) (succ x)\",\n",
    "  \"@[simp] theorem <a>Nat.mod_self</a> (n : Nat) : n % n = 0\",\n",
    "]\n",
    "input = \"\\n\\n\".join(retrieved_premises + [state])\n",
    "print(\"------ INPUT ------\\n\", input)\n",
    "tokenized_input = tokenizer(input, return_tensors=\"pt\", max_length=2300, truncation=True)\n",
    "\n",
    "# Generate a single tactic.\n",
    "tactic_ids = model.generate(tokenized_input.input_ids, max_length=1024)\n",
    "tactic = tokenizer.decode(tactic_ids[0], skip_special_tokens=True)\n",
    "print(\"\\n------ OUTPUT ------\")\n",
    "print(tactic, end=\"\\n\\n\")\n",
    "\n",
    "# Generate multiple tactics via beam search.\n",
    "tactic_candidates_ids = model.generate(\n",
    "    tokenized_input.input_ids,\n",
    "    max_length=1024,\n",
    "    num_beams=4,\n",
    "    length_penalty=0.0,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=4,\n",
    "    early_stopping=False,\n",
    ")\n",
    "tactic_candidates = tokenizer.batch_decode(\n",
    "    tactic_candidates_ids, skip_special_tokens=True\n",
    ")\n",
    "for tac in tactic_candidates:\n",
    "    print(tac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e55f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create indexed corpus on all premises (with their embeddings)\n",
    "python retrieval/index.py \\\n",
    "--ckpt_path /home/mcwave/code/3rdparty/ReProver/ckpt/leandojo-pl-ckpts/retriver_novel_premises.ckpt \\\n",
    "--corpus-path /home/mcwave/code/3rdparty/ReProver/data/leandojo_benchmark_4/corpus.jsonl \\\n",
    "--output-path /home/mcwave/code/3rdparty/ReProver/data/indexed_corpus\n",
    "\n",
    "\n",
    "# python retrieval/index.py \\\n",
    "# --ckpt_path /home/mcwave/code/3rdparty/ReProver/lightning_logs/version_2/checkpoints/last.ckpt \\\n",
    "# --corpus-path /home/mcwave/code/3rdparty/ReProver/data/leandojo_benchmark_4/corpus.jsonl \\\n",
    "# --output-path /home/mcwave/code/3rdparty/ReProver/data/indexed_corpus\n",
    "\n",
    "\n",
    "python prover/evaluate.py \\\n",
    "--data-path /home/mcwave/code/3rdparty/ReProver/data/leandojo_benchmark_4/novel_premises/  \\\n",
    "--ckpt_path /home/mcwave/code/3rdparty/ReProver/ckpt/leandojo-pl-ckpts/reprover_novel_premises.ckpt \\\n",
    "--indexed-corpus-path /home/mcwave/code/3rdparty/ReProver/data/indexed_corpus \\\n",
    "--split test --num-workers 5 --num-gpus 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fef9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Theorem Proving Evaluation on LeanDojo Benchmark\n",
    "After the tactic generator is trained, we combine it with best-first search to prove theorems by interacting with Lean.\n",
    "\n",
    "For models without retrieval, run:\n",
    "\"\"\"\n",
    "\n",
    "python prover/evaluate.py \\\n",
    "--data-path /home/mcwave/code/3rdparty/ReProver/data/leandojo_benchmark_4/novel_premises/ \\\n",
    "--ckpt_path /home/mcwave/code/3rdparty/ReProver/ckpt/leandojo-pl-ckpts/generator_novel_premises.ckpt \\\n",
    "--split test --num-workers 1 --num-gpus 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atp",
   "language": "python",
   "name": "atp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
