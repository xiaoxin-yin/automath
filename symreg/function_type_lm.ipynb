{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8de7088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.         -3.99997287 -3.99994575 ...  3.99991862  3.99994575\n",
      "  3.99997287]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seq': tensor([[-4.0000, -4.0000, -3.9999,  ..., -3.9897, -3.9896, -3.9896],\n",
       "         [-3.9896, -3.9896, -3.9895,  ..., -3.9792, -3.9792, -3.9792],\n",
       "         [-3.9792, -3.9791, -3.9791,  ..., -3.9688, -3.9688, -3.9688],\n",
       "         ...,\n",
       "         [ 3.9688,  3.9688,  3.9688,  ...,  3.9791,  3.9791,  3.9791],\n",
       "         [ 3.9792,  3.9792,  3.9792,  ...,  3.9895,  3.9895,  3.9896],\n",
       "         [ 3.9896,  3.9896,  3.9896,  ...,  3.9999,  3.9999,  4.0000]]),\n",
       " 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sympy as sp\n",
    "from sympy import sympify, lambdify, symbols, integrate, Interval, Symbol, I, S, oo, plot\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "TRAIN_SIZE = 1000000\n",
    "VALID_SIZE = 10000\n",
    "SEQ_LEN = 768\n",
    "OUTPUT_LEN = 256\n",
    "EMBED_DIM = 384\n",
    "\n",
    "\n",
    "def generate_sample(f, min_x=-4, max_x=4):\n",
    "    increment = (max_x-min_x)/SEQ_LEN/EMBED_DIM\n",
    "    x, t = symbols(['x','t'])\n",
    "    fl = lambdify((t), f, \"numpy\")\n",
    "    xs = np.arange(min_x, max_x, increment)\n",
    "    ys = fl(xs)\n",
    "    if isinstance(ys, float) or isinstance(ys, int):\n",
    "        ys = np.full(len(xs), float(ys))\n",
    "    if np.isnan(ys).any() or np.isinf(ys).any():\n",
    "        print(\"Error! NaN or Inf found!\")\n",
    "        ys = np.zeros(ys.shape())\n",
    "    return xs, ys\n",
    "\n",
    "def sample2seq(raw_sample, seq_len, embed_dim, seq_first=True):\n",
    "    #seq = torch.zeros(seq_len, embed_dim)\n",
    "    attention_mask = torch.ones(seq_len) # May change to only where values are present\n",
    "    if seq_first:\n",
    "        tmp = np.reshape(raw_sample, (seq_len, embed_dim)).astype(np.float32)\n",
    "        seq = torch.from_numpy(tmp)\n",
    "    else:\n",
    "        tmp = np.transpose(np.reshape(raw_sample, (embed_dim, seq_len))).astype(np.float32)\n",
    "    return {\"seq\":seq, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "f = sympify(\"t**2\")\n",
    "xs, ys = generate_sample(f)\n",
    "print(xs)\n",
    "\n",
    "sample2seq(xs, SEQ_LEN, EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "\n",
    "# def remove_constants(f):\n",
    "#     t = Symbol('t')\n",
    "#     return f.as_independent(t)[1]\n",
    "\n",
    "# fin = open(\"/home/mcwave/code/automath/calculus/datasets/parametric_equations_polynomial_integral_results.json\", \"r\")\n",
    "# lines = fin.readlines()\n",
    "# print(len(lines), \"lines loaded\")\n",
    "# fin.close()\n",
    "# fin = open(\"/home/mcwave/code/automath/calculus/datasets/parametric_equations_randomized_polynomial_integral_results.json\", \"r\")\n",
    "# lines.extend(fin.readlines())\n",
    "# print(len(lines), \"lines loaded\")\n",
    "# fin.close()\n",
    "# fin = open(\"/home/mcwave/code/automath/calculus/datasets/parametric_equations_randomized_nonpoly_integral_results_corrected.json\", \"r\")\n",
    "# lines.extend(fin.readlines())\n",
    "# print(len(lines), \"lines loaded\")\n",
    "# fin.close()\n",
    "\n",
    "# MAX_POWER = 6\n",
    "# MAX_AVG_DIFF = 0.01\n",
    "\n",
    "# originals = []\n",
    "\n",
    "# for line in lines:\n",
    "#     result = json.loads(line)\n",
    "#     original = result[\"original\"]\n",
    "#     originals.append(original)\n",
    "#     if len(originals) % 1000 == 0:\n",
    "#         print(len(originals), \"cases loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "421e019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 rows processed\n",
      "2000 rows processed\n",
      "3000 rows processed\n",
      "4000 rows processed\n",
      "5000 rows processed\n",
      "6000 rows processed\n",
      "7000 rows processed\n",
      "8000 rows processed\n",
      "9000 rows processed\n",
      "10000 rows processed\n",
      "11000 rows processed\n",
      "12000 rows processed\n",
      "13000 rows processed\n",
      "14000 rows processed\n",
      "15000 rows processed\n",
      "16000 rows processed\n",
      "17000 rows processed\n",
      "18000 rows processed\n",
      "19000 rows processed\n",
      "20000 rows processed\n",
      "21000 rows processed\n",
      "22000 rows processed\n",
      "23000 rows processed\n",
      "24000 rows processed\n",
      "25000 rows processed\n",
      "26000 rows processed\n",
      "27000 rows processed\n",
      "28000 rows processed\n",
      "29000 rows processed\n",
      "30000 rows processed\n",
      "31000 rows processed\n",
      "32000 rows processed\n",
      "33000 rows processed\n",
      "34000 rows processed\n",
      "35000 rows processed\n",
      "36000 rows processed\n",
      "37000 rows processed\n",
      "38000 rows processed\n",
      "39000 rows processed\n",
      "40000 rows processed\n",
      "41000 rows processed\n",
      "42000 rows processed\n",
      "43000 rows processed\n",
      "44000 rows processed\n",
      "45000 rows processed\n",
      "46000 rows processed\n",
      "47000 rows processed\n",
      "48000 rows processed\n",
      "49000 rows processed\n",
      "50000 rows processed\n",
      "51000 rows processed\n",
      "52000 rows processed\n",
      "53000 rows processed\n",
      "54000 rows processed\n",
      "55000 rows processed\n",
      "56000 rows processed\n",
      "57000 rows processed\n",
      "58000 rows processed\n",
      "59000 rows processed\n",
      "60000 rows processed\n",
      "61000 rows processed\n",
      "62000 rows processed\n",
      "63000 rows processed\n",
      "64000 rows processed\n",
      "65000 rows processed\n",
      "66000 rows processed\n",
      "67000 rows processed\n",
      "68000 rows processed\n",
      "69000 rows processed\n",
      "70000 rows processed\n",
      "71000 rows processed\n",
      "72000 rows processed\n",
      "73000 rows processed\n",
      "74000 rows processed\n",
      "75000 rows processed\n",
      "76000 rows processed\n",
      "77000 rows processed\n",
      "78000 rows processed\n",
      "79000 rows processed\n",
      "80000 rows processed\n",
      "81000 rows processed\n",
      "82000 rows processed\n",
      "83000 rows processed\n",
      "84000 rows processed\n",
      "85000 rows processed\n",
      "86000 rows processed\n",
      "87000 rows processed\n",
      "88000 rows processed\n",
      "89000 rows processed\n",
      "90000 rows processed\n",
      "91000 rows processed\n",
      "92000 rows processed\n",
      "93000 rows processed\n",
      "94000 rows processed\n",
      "95000 rows processed\n",
      "96000 rows processed\n",
      "97000 rows processed\n",
      "98000 rows processed\n",
      "99000 rows processed\n",
      "100000 rows processed\n",
      "101000 rows processed\n",
      "102000 rows processed\n",
      "103000 rows processed\n",
      "104000 rows processed\n",
      "105000 rows processed\n",
      "106000 rows processed\n",
      "107000 rows processed\n",
      "108000 rows processed\n",
      "109000 rows processed\n",
      "110000 rows processed\n",
      "111000 rows processed\n",
      "112000 rows processed\n",
      "113000 rows processed\n",
      "114000 rows processed\n",
      "115000 rows processed\n",
      "116000 rows processed\n",
      "117000 rows processed\n",
      "118000 rows processed\n",
      "119000 rows processed\n",
      "120000 rows processed\n",
      "121000 rows processed\n",
      "122000 rows processed\n",
      "123000 rows processed\n",
      "124000 rows processed\n",
      "125000 rows processed\n",
      "126000 rows processed\n",
      "127000 rows processed\n",
      "128000 rows processed\n",
      "129000 rows processed\n",
      "130000 rows processed\n",
      "131000 rows processed\n",
      "132000 rows processed\n",
      "133000 rows processed\n",
      "134000 rows processed\n",
      "135000 rows processed\n",
      "136000 rows processed\n",
      "137000 rows processed\n",
      "138000 rows processed\n",
      "139000 rows processed\n",
      "140000 rows processed\n",
      "141000 rows processed\n",
      "142000 rows processed\n",
      "143000 rows processed\n",
      "144000 rows processed\n",
      "145000 rows processed\n",
      "146000 rows processed\n",
      "147000 rows processed\n",
      "148000 rows processed\n",
      "149000 rows processed\n",
      "150000 rows processed\n",
      "151000 rows processed\n",
      "152000 rows processed\n",
      "153000 rows processed\n",
      "154000 rows processed\n",
      "155000 rows processed\n",
      "156000 rows processed\n",
      "157000 rows processed\n",
      "158000 rows processed\n",
      "159000 rows processed\n",
      "160000 rows processed\n",
      "161000 rows processed\n",
      "162000 rows processed\n",
      "163000 rows processed\n",
      "164000 rows processed\n",
      "165000 rows processed\n",
      "166000 rows processed\n",
      "167000 rows processed\n",
      "168000 rows processed\n",
      "169000 rows processed\n",
      "170000 rows processed\n",
      "171000 rows processed\n",
      "172000 rows processed\n",
      "173000 rows processed\n",
      "174000 rows processed\n",
      "175000 rows processed\n",
      "176000 rows processed\n",
      "177000 rows processed\n",
      "178000 rows processed\n",
      "179000 rows processed\n",
      "180000 rows processed\n",
      "181000 rows processed\n",
      "182000 rows processed\n",
      "183000 rows processed\n",
      "184000 rows processed\n",
      "185000 rows processed\n",
      "186000 rows processed\n",
      "187000 rows processed\n",
      "188000 rows processed\n",
      "189000 rows processed\n",
      "190000 rows processed\n",
      "191000 rows processed\n",
      "192000 rows processed\n",
      "193000 rows processed\n",
      "194000 rows processed\n",
      "195000 rows processed\n",
      "196000 rows processed\n",
      "197000 rows processed\n",
      "198000 rows processed\n",
      "199000 rows processed\n",
      "200000 rows processed\n",
      "201000 rows processed\n",
      "202000 rows processed\n",
      "203000 rows processed\n",
      "204000 rows processed\n",
      "205000 rows processed\n",
      "206000 rows processed\n",
      "207000 rows processed\n",
      "208000 rows processed\n",
      "209000 rows processed\n",
      "210000 rows processed\n",
      "211000 rows processed\n",
      "212000 rows processed\n",
      "213000 rows processed\n",
      "214000 rows processed\n",
      "215000 rows processed\n",
      "216000 rows processed\n",
      "217000 rows processed\n",
      "218000 rows processed\n",
      "219000 rows processed\n",
      "220000 rows processed\n",
      "221000 rows processed\n",
      "222000 rows processed\n",
      "223000 rows processed\n",
      "224000 rows processed\n",
      "225000 rows processed\n",
      "226000 rows processed\n",
      "227000 rows processed\n",
      "228000 rows processed\n",
      "229000 rows processed\n",
      "230000 rows processed\n",
      "231000 rows processed\n",
      "232000 rows processed\n",
      "233000 rows processed\n",
      "234000 rows processed\n",
      "235000 rows processed\n",
      "236000 rows processed\n",
      "237000 rows processed\n",
      "238000 rows processed\n",
      "239000 rows processed\n",
      "240000 rows processed\n",
      "241000 rows processed\n",
      "242000 rows processed\n",
      "243000 rows processed\n",
      "244000 rows processed\n",
      "245000 rows processed\n",
      "246000 rows processed\n",
      "247000 rows processed\n",
      "248000 rows processed\n",
      "249000 rows processed\n",
      "250000 rows processed\n",
      "251000 rows processed\n",
      "252000 rows processed\n",
      "253000 rows processed\n",
      "254000 rows processed\n",
      "255000 rows processed\n",
      "256000 rows processed\n",
      "257000 rows processed\n",
      "258000 rows processed\n",
      "259000 rows processed\n",
      "260000 rows processed\n",
      "261000 rows processed\n",
      "262000 rows processed\n",
      "263000 rows processed\n",
      "264000 rows processed\n",
      "265000 rows processed\n",
      "266000 rows processed\n",
      "267000 rows processed\n",
      "268000 rows processed\n",
      "269000 rows processed\n",
      "270000 rows processed\n",
      "271000 rows processed\n",
      "272000 rows processed\n",
      "273000 rows processed\n",
      "274000 rows processed\n",
      "275000 rows processed\n",
      "276000 rows processed\n",
      "277000 rows processed\n",
      "278000 rows processed\n",
      "279000 rows processed\n",
      "280000 rows processed\n",
      "281000 rows processed\n",
      "282000 rows processed\n",
      "283000 rows processed\n",
      "284000 rows processed\n",
      "285000 rows processed\n",
      "286000 rows processed\n",
      "287000 rows processed\n",
      "288000 rows processed\n",
      "289000 rows processed\n",
      "290000 rows processed\n",
      "291000 rows processed\n",
      "292000 rows processed\n",
      "293000 rows processed\n",
      "294000 rows processed\n",
      "295000 rows processed\n",
      "296000 rows processed\n",
      "297000 rows processed\n",
      "298000 rows processed\n",
      "299000 rows processed\n",
      "300000 rows processed\n",
      "301000 rows processed\n",
      "302000 rows processed\n",
      "303000 rows processed\n",
      "304000 rows processed\n",
      "305000 rows processed\n",
      "306000 rows processed\n",
      "307000 rows processed\n",
      "308000 rows processed\n",
      "309000 rows processed\n",
      "310000 rows processed\n",
      "311000 rows processed\n",
      "312000 rows processed\n",
      "313000 rows processed\n",
      "314000 rows processed\n",
      "315000 rows processed\n",
      "316000 rows processed\n",
      "317000 rows processed\n",
      "318000 rows processed\n",
      "319000 rows processed\n",
      "320000 rows processed\n",
      "321000 rows processed\n",
      "322000 rows processed\n",
      "323000 rows processed\n",
      "324000 rows processed\n",
      "325000 rows processed\n",
      "326000 rows processed\n",
      "327000 rows processed\n",
      "328000 rows processed\n",
      "329000 rows processed\n",
      "330000 rows processed\n",
      "331000 rows processed\n",
      "332000 rows processed\n",
      "333000 rows processed\n",
      "334000 rows processed\n",
      "335000 rows processed\n",
      "336000 rows processed\n",
      "337000 rows processed\n",
      "338000 rows processed\n",
      "339000 rows processed\n",
      "340000 rows processed\n",
      "341000 rows processed\n",
      "342000 rows processed\n",
      "343000 rows processed\n",
      "344000 rows processed\n",
      "345000 rows processed\n",
      "346000 rows processed\n",
      "347000 rows processed\n",
      "348000 rows processed\n",
      "349000 rows processed\n",
      "350000 rows processed\n",
      "351000 rows processed\n",
      "352000 rows processed\n",
      "353000 rows processed\n",
      "354000 rows processed\n",
      "355000 rows processed\n",
      "356000 rows processed\n",
      "357000 rows processed\n",
      "358000 rows processed\n",
      "359000 rows processed\n",
      "360000 rows processed\n",
      "361000 rows processed\n",
      "362000 rows processed\n",
      "363000 rows processed\n",
      "364000 rows processed\n",
      "365000 rows processed\n",
      "366000 rows processed\n",
      "367000 rows processed\n",
      "368000 rows processed\n",
      "369000 rows processed\n",
      "370000 rows processed\n",
      "371000 rows processed\n",
      "372000 rows processed\n",
      "373000 rows processed\n",
      "374000 rows processed\n",
      "375000 rows processed\n",
      "376000 rows processed\n",
      "377000 rows processed\n",
      "378000 rows processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379000 rows processed\n",
      "380000 rows processed\n",
      "381000 rows processed\n",
      "382000 rows processed\n",
      "383000 rows processed\n",
      "384000 rows processed\n",
      "385000 rows processed\n",
      "386000 rows processed\n",
      "387000 rows processed\n",
      "388000 rows processed\n",
      "389000 rows processed\n",
      "390000 rows processed\n",
      "391000 rows processed\n",
      "392000 rows processed\n",
      "393000 rows processed\n",
      "394000 rows processed\n",
      "395000 rows processed\n",
      "396000 rows processed\n",
      "397000 rows processed\n",
      "398000 rows processed\n",
      "399000 rows processed\n",
      "400000 rows processed\n",
      "401000 rows processed\n",
      "402000 rows processed\n",
      "403000 rows processed\n",
      "404000 rows processed\n",
      "405000 rows processed\n",
      "406000 rows processed\n",
      "407000 rows processed\n",
      "408000 rows processed\n",
      "409000 rows processed\n",
      "410000 rows processed\n",
      "411000 rows processed\n",
      "412000 rows processed\n",
      "413000 rows processed\n",
      "414000 rows processed\n",
      "415000 rows processed\n",
      "416000 rows processed\n",
      "417000 rows processed\n",
      "418000 rows processed\n",
      "419000 rows processed\n",
      "420000 rows processed\n",
      "421000 rows processed\n",
      "422000 rows processed\n",
      "423000 rows processed\n",
      "424000 rows processed\n",
      "425000 rows processed\n",
      "426000 rows processed\n",
      "427000 rows processed\n",
      "428000 rows processed\n",
      "429000 rows processed\n",
      "430000 rows processed\n",
      "431000 rows processed\n",
      "432000 rows processed\n",
      "433000 rows processed\n",
      "434000 rows processed\n",
      "435000 rows processed\n",
      "436000 rows processed\n",
      "437000 rows processed\n",
      "438000 rows processed\n",
      "439000 rows processed\n",
      "440000 rows processed\n",
      "441000 rows processed\n",
      "442000 rows processed\n",
      "443000 rows processed\n",
      "444000 rows processed\n",
      "445000 rows processed\n",
      "446000 rows processed\n",
      "447000 rows processed\n",
      "448000 rows processed\n",
      "449000 rows processed\n",
      "450000 rows processed\n",
      "451000 rows processed\n",
      "452000 rows processed\n",
      "453000 rows processed\n",
      "454000 rows processed\n",
      "455000 rows processed\n",
      "456000 rows processed\n",
      "457000 rows processed\n",
      "458000 rows processed\n",
      "459000 rows processed\n",
      "460000 rows processed\n",
      "461000 rows processed\n",
      "462000 rows processed\n",
      "463000 rows processed\n",
      "464000 rows processed\n",
      "465000 rows processed\n",
      "466000 rows processed\n",
      "467000 rows processed\n",
      "468000 rows processed\n",
      "469000 rows processed\n",
      "470000 rows processed\n",
      "471000 rows processed\n",
      "472000 rows processed\n",
      "473000 rows processed\n",
      "474000 rows processed\n",
      "475000 rows processed\n",
      "476000 rows processed\n",
      "477000 rows processed\n",
      "478000 rows processed\n",
      "479000 rows processed\n",
      "480000 rows processed\n",
      "481000 rows processed\n",
      "482000 rows processed\n",
      "483000 rows processed\n",
      "484000 rows processed\n",
      "485000 rows processed\n",
      "486000 rows processed\n",
      "487000 rows processed\n",
      "488000 rows processed\n",
      "489000 rows processed\n",
      "490000 rows processed\n",
      "491000 rows processed\n",
      "492000 rows processed\n",
      "493000 rows processed\n",
      "494000 rows processed\n",
      "495000 rows processed\n",
      "496000 rows processed\n",
      "497000 rows processed\n",
      "498000 rows processed\n",
      "499000 rows processed\n",
      "500000 rows processed\n",
      "501000 rows processed\n",
      "502000 rows processed\n",
      "503000 rows processed\n",
      "504000 rows processed\n",
      "505000 rows processed\n",
      "506000 rows processed\n",
      "507000 rows processed\n",
      "508000 rows processed\n",
      "509000 rows processed\n",
      "510000 rows processed\n",
      "511000 rows processed\n",
      "512000 rows processed\n",
      "513000 rows processed\n",
      "514000 rows processed\n",
      "515000 rows processed\n",
      "516000 rows processed\n",
      "517000 rows processed\n",
      "518000 rows processed\n",
      "519000 rows processed\n",
      "520000 rows processed\n",
      "521000 rows processed\n",
      "522000 rows processed\n",
      "523000 rows processed\n",
      "524000 rows processed\n",
      "525000 rows processed\n",
      "526000 rows processed\n",
      "527000 rows processed\n",
      "528000 rows processed\n",
      "529000 rows processed\n",
      "530000 rows processed\n",
      "531000 rows processed\n",
      "532000 rows processed\n",
      "533000 rows processed\n",
      "534000 rows processed\n",
      "535000 rows processed\n",
      "536000 rows processed\n",
      "537000 rows processed\n",
      "538000 rows processed\n",
      "539000 rows processed\n",
      "540000 rows processed\n",
      "541000 rows processed\n",
      "542000 rows processed\n",
      "543000 rows processed\n",
      "544000 rows processed\n",
      "545000 rows processed\n",
      "546000 rows processed\n",
      "547000 rows processed\n",
      "548000 rows processed\n",
      "549000 rows processed\n",
      "550000 rows processed\n",
      "551000 rows processed\n",
      "552000 rows processed\n",
      "553000 rows processed\n",
      "554000 rows processed\n",
      "555000 rows processed\n",
      "556000 rows processed\n",
      "557000 rows processed\n",
      "558000 rows processed\n",
      "559000 rows processed\n",
      "560000 rows processed\n",
      "561000 rows processed\n",
      "562000 rows processed\n",
      "563000 rows processed\n",
      "564000 rows processed\n",
      "565000 rows processed\n",
      "566000 rows processed\n",
      "567000 rows processed\n",
      "568000 rows processed\n",
      "569000 rows processed\n",
      "570000 rows processed\n",
      "571000 rows processed\n",
      "572000 rows processed\n",
      "573000 rows processed\n",
      "574000 rows processed\n",
      "575000 rows processed\n",
      "576000 rows processed\n",
      "577000 rows processed\n",
      "578000 rows processed\n",
      "579000 rows processed\n",
      "580000 rows processed\n",
      "581000 rows processed\n",
      "582000 rows processed\n",
      "583000 rows processed\n",
      "584000 rows processed\n",
      "585000 rows processed\n",
      "586000 rows processed\n",
      "587000 rows processed\n",
      "588000 rows processed\n",
      "589000 rows processed\n",
      "590000 rows processed\n",
      "591000 rows processed\n",
      "592000 rows processed\n",
      "593000 rows processed\n",
      "594000 rows processed\n",
      "595000 rows processed\n",
      "596000 rows processed\n",
      "597000 rows processed\n",
      "598000 rows processed\n",
      "599000 rows processed\n",
      "600000 rows processed\n",
      "601000 rows processed\n",
      "602000 rows processed\n",
      "603000 rows processed\n",
      "604000 rows processed\n",
      "605000 rows processed\n",
      "606000 rows processed\n",
      "607000 rows processed\n",
      "608000 rows processed\n",
      "609000 rows processed\n",
      "610000 rows processed\n",
      "611000 rows processed\n",
      "612000 rows processed\n",
      "613000 rows processed\n",
      "614000 rows processed\n",
      "615000 rows processed\n",
      "616000 rows processed\n",
      "617000 rows processed\n",
      "618000 rows processed\n",
      "619000 rows processed\n",
      "620000 rows processed\n",
      "621000 rows processed\n",
      "622000 rows processed\n",
      "623000 rows processed\n",
      "624000 rows processed\n",
      "625000 rows processed\n",
      "626000 rows processed\n",
      "627000 rows processed\n",
      "628000 rows processed\n",
      "629000 rows processed\n",
      "630000 rows processed\n",
      "631000 rows processed\n",
      "632000 rows processed\n",
      "633000 rows processed\n",
      "634000 rows processed\n",
      "635000 rows processed\n",
      "636000 rows processed\n",
      "637000 rows processed\n",
      "638000 rows processed\n",
      "639000 rows processed\n",
      "640000 rows processed\n",
      "641000 rows processed\n",
      "642000 rows processed\n",
      "643000 rows processed\n",
      "644000 rows processed\n",
      "645000 rows processed\n",
      "646000 rows processed\n",
      "647000 rows processed\n",
      "648000 rows processed\n",
      "649000 rows processed\n",
      "650000 rows processed\n",
      "651000 rows processed\n",
      "652000 rows processed\n",
      "653000 rows processed\n",
      "654000 rows processed\n",
      "655000 rows processed\n",
      "656000 rows processed\n",
      "657000 rows processed\n",
      "658000 rows processed\n",
      "659000 rows processed\n",
      "660000 rows processed\n",
      "661000 rows processed\n",
      "662000 rows processed\n",
      "663000 rows processed\n",
      "664000 rows processed\n",
      "665000 rows processed\n",
      "666000 rows processed\n",
      "667000 rows processed\n",
      "668000 rows processed\n",
      "669000 rows processed\n",
      "670000 rows processed\n",
      "671000 rows processed\n",
      "672000 rows processed\n",
      "673000 rows processed\n",
      "674000 rows processed\n",
      "675000 rows processed\n",
      "676000 rows processed\n",
      "677000 rows processed\n",
      "678000 rows processed\n",
      "679000 rows processed\n",
      "680000 rows processed\n",
      "681000 rows processed\n",
      "682000 rows processed\n",
      "683000 rows processed\n",
      "684000 rows processed\n",
      "685000 rows processed\n",
      "686000 rows processed\n",
      "687000 rows processed\n",
      "688000 rows processed\n",
      "689000 rows processed\n",
      "690000 rows processed\n",
      "691000 rows processed\n",
      "692000 rows processed\n",
      "693000 rows processed\n",
      "694000 rows processed\n",
      "695000 rows processed\n",
      "696000 rows processed\n",
      "697000 rows processed\n",
      "698000 rows processed\n",
      "699000 rows processed\n",
      "700000 rows processed\n",
      "701000 rows processed\n",
      "702000 rows processed\n",
      "703000 rows processed\n",
      "704000 rows processed\n",
      "705000 rows processed\n",
      "706000 rows processed\n",
      "707000 rows processed\n",
      "708000 rows processed\n",
      "709000 rows processed\n",
      "710000 rows processed\n",
      "711000 rows processed\n",
      "712000 rows processed\n",
      "713000 rows processed\n",
      "714000 rows processed\n",
      "715000 rows processed\n",
      "716000 rows processed\n",
      "717000 rows processed\n",
      "718000 rows processed\n",
      "719000 rows processed\n",
      "720000 rows processed\n",
      "721000 rows processed\n",
      "722000 rows processed\n",
      "723000 rows processed\n",
      "724000 rows processed\n",
      "725000 rows processed\n",
      "726000 rows processed\n",
      "727000 rows processed\n",
      "728000 rows processed\n",
      "729000 rows processed\n",
      "730000 rows processed\n",
      "731000 rows processed\n",
      "732000 rows processed\n",
      "733000 rows processed\n",
      "734000 rows processed\n",
      "735000 rows processed\n",
      "736000 rows processed\n",
      "737000 rows processed\n",
      "738000 rows processed\n",
      "739000 rows processed\n",
      "740000 rows processed\n",
      "741000 rows processed\n",
      "742000 rows processed\n",
      "743000 rows processed\n",
      "744000 rows processed\n",
      "745000 rows processed\n",
      "746000 rows processed\n",
      "747000 rows processed\n",
      "748000 rows processed\n",
      "749000 rows processed\n",
      "750000 rows processed\n",
      "751000 rows processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752000 rows processed\n",
      "753000 rows processed\n",
      "754000 rows processed\n",
      "755000 rows processed\n",
      "756000 rows processed\n",
      "757000 rows processed\n",
      "758000 rows processed\n",
      "759000 rows processed\n",
      "760000 rows processed\n",
      "761000 rows processed\n",
      "762000 rows processed\n",
      "763000 rows processed\n",
      "764000 rows processed\n",
      "765000 rows processed\n",
      "766000 rows processed\n",
      "767000 rows processed\n",
      "768000 rows processed\n",
      "769000 rows processed\n",
      "770000 rows processed\n",
      "771000 rows processed\n",
      "772000 rows processed\n",
      "773000 rows processed\n",
      "774000 rows processed\n",
      "775000 rows processed\n",
      "776000 rows processed\n",
      "777000 rows processed\n",
      "778000 rows processed\n",
      "779000 rows processed\n",
      "780000 rows processed\n",
      "781000 rows processed\n",
      "782000 rows processed\n",
      "783000 rows processed\n",
      "784000 rows processed\n",
      "785000 rows processed\n",
      "786000 rows processed\n",
      "787000 rows processed\n",
      "788000 rows processed\n",
      "789000 rows processed\n",
      "790000 rows processed\n",
      "791000 rows processed\n",
      "792000 rows processed\n",
      "793000 rows processed\n",
      "794000 rows processed\n",
      "795000 rows processed\n",
      "796000 rows processed\n",
      "797000 rows processed\n",
      "798000 rows processed\n",
      "799000 rows processed\n",
      "800000 rows processed\n",
      "801000 rows processed\n",
      "802000 rows processed\n",
      "803000 rows processed\n",
      "804000 rows processed\n",
      "805000 rows processed\n",
      "806000 rows processed\n",
      "807000 rows processed\n",
      "808000 rows processed\n",
      "809000 rows processed\n",
      "810000 rows processed\n",
      "811000 rows processed\n",
      "812000 rows processed\n",
      "813000 rows processed\n",
      "814000 rows processed\n",
      "815000 rows processed\n",
      "816000 rows processed\n",
      "817000 rows processed\n",
      "818000 rows processed\n",
      "819000 rows processed\n",
      "820000 rows processed\n",
      "821000 rows processed\n",
      "822000 rows processed\n",
      "823000 rows processed\n",
      "824000 rows processed\n",
      "825000 rows processed\n",
      "826000 rows processed\n",
      "827000 rows processed\n",
      "828000 rows processed\n",
      "829000 rows processed\n",
      "830000 rows processed\n",
      "831000 rows processed\n",
      "832000 rows processed\n",
      "833000 rows processed\n",
      "834000 rows processed\n",
      "835000 rows processed\n",
      "836000 rows processed\n",
      "837000 rows processed\n",
      "838000 rows processed\n",
      "839000 rows processed\n",
      "840000 rows processed\n",
      "841000 rows processed\n",
      "842000 rows processed\n",
      "843000 rows processed\n",
      "844000 rows processed\n",
      "845000 rows processed\n",
      "846000 rows processed\n",
      "847000 rows processed\n",
      "848000 rows processed\n",
      "849000 rows processed\n",
      "850000 rows processed\n",
      "851000 rows processed\n",
      "852000 rows processed\n",
      "853000 rows processed\n",
      "854000 rows processed\n",
      "855000 rows processed\n",
      "856000 rows processed\n",
      "857000 rows processed\n",
      "858000 rows processed\n",
      "859000 rows processed\n",
      "860000 rows processed\n",
      "861000 rows processed\n",
      "862000 rows processed\n",
      "863000 rows processed\n",
      "864000 rows processed\n",
      "865000 rows processed\n",
      "866000 rows processed\n",
      "867000 rows processed\n",
      "868000 rows processed\n",
      "869000 rows processed\n",
      "870000 rows processed\n",
      "871000 rows processed\n",
      "872000 rows processed\n",
      "873000 rows processed\n",
      "874000 rows processed\n",
      "875000 rows processed\n",
      "876000 rows processed\n",
      "877000 rows processed\n",
      "878000 rows processed\n",
      "879000 rows processed\n",
      "880000 rows processed\n",
      "881000 rows processed\n",
      "882000 rows processed\n",
      "883000 rows processed\n",
      "884000 rows processed\n",
      "885000 rows processed\n",
      "886000 rows processed\n",
      "887000 rows processed\n",
      "888000 rows processed\n",
      "889000 rows processed\n",
      "890000 rows processed\n",
      "891000 rows processed\n",
      "892000 rows processed\n",
      "893000 rows processed\n",
      "894000 rows processed\n",
      "895000 rows processed\n",
      "896000 rows processed\n",
      "897000 rows processed\n",
      "898000 rows processed\n",
      "899000 rows processed\n",
      "900000 rows processed\n",
      "901000 rows processed\n",
      "902000 rows processed\n",
      "903000 rows processed\n",
      "904000 rows processed\n",
      "905000 rows processed\n",
      "906000 rows processed\n",
      "907000 rows processed\n",
      "908000 rows processed\n",
      "909000 rows processed\n",
      "910000 rows processed\n",
      "911000 rows processed\n",
      "912000 rows processed\n",
      "913000 rows processed\n",
      "914000 rows processed\n",
      "915000 rows processed\n",
      "916000 rows processed\n",
      "917000 rows processed\n",
      "918000 rows processed\n",
      "919000 rows processed\n",
      "920000 rows processed\n",
      "921000 rows processed\n",
      "922000 rows processed\n",
      "923000 rows processed\n",
      "924000 rows processed\n",
      "925000 rows processed\n",
      "926000 rows processed\n",
      "927000 rows processed\n",
      "928000 rows processed\n",
      "929000 rows processed\n",
      "930000 rows processed\n",
      "931000 rows processed\n",
      "932000 rows processed\n",
      "933000 rows processed\n",
      "934000 rows processed\n",
      "935000 rows processed\n",
      "936000 rows processed\n",
      "937000 rows processed\n",
      "938000 rows processed\n",
      "939000 rows processed\n",
      "940000 rows processed\n",
      "941000 rows processed\n",
      "942000 rows processed\n",
      "943000 rows processed\n",
      "944000 rows processed\n",
      "945000 rows processed\n",
      "946000 rows processed\n",
      "947000 rows processed\n",
      "948000 rows processed\n",
      "949000 rows processed\n",
      "950000 rows processed\n",
      "951000 rows processed\n",
      "952000 rows processed\n",
      "953000 rows processed\n",
      "954000 rows processed\n",
      "955000 rows processed\n",
      "956000 rows processed\n",
      "957000 rows processed\n",
      "958000 rows processed\n",
      "959000 rows processed\n",
      "960000 rows processed\n",
      "961000 rows processed\n",
      "962000 rows processed\n",
      "963000 rows processed\n",
      "964000 rows processed\n",
      "965000 rows processed\n",
      "966000 rows processed\n",
      "967000 rows processed\n",
      "968000 rows processed\n",
      "969000 rows processed\n",
      "970000 rows processed\n",
      "971000 rows processed\n",
      "972000 rows processed\n",
      "973000 rows processed\n",
      "974000 rows processed\n",
      "975000 rows processed\n",
      "976000 rows processed\n",
      "977000 rows processed\n",
      "978000 rows processed\n",
      "979000 rows processed\n",
      "980000 rows processed\n",
      "981000 rows processed\n",
      "982000 rows processed\n",
      "983000 rows processed\n",
      "984000 rows processed\n",
      "985000 rows processed\n",
      "986000 rows processed\n",
      "987000 rows processed\n",
      "988000 rows processed\n",
      "989000 rows processed\n",
      "990000 rows processed\n",
      "991000 rows processed\n",
      "992000 rows processed\n",
      "993000 rows processed\n",
      "994000 rows processed\n",
      "995000 rows processed\n",
      "996000 rows processed\n",
      "997000 rows processed\n",
      "998000 rows processed\n",
      "999000 rows processed\n",
      "1000000 rows processed\n"
     ]
    }
   ],
   "source": [
    "# NUM_LABELS = 10\n",
    "# MAX_POWER = 6\n",
    "# FUNCTIONS = {'exp':7, 'sin':8, 'cos':9}\n",
    "\n",
    "# def get_coefficients_and_exponents(f):\n",
    "#     variables = list(f.free_symbols)\n",
    "#     assert len(variables)<=1, \"Expression having multiple variable \" + str(f)\n",
    "#     if len(variables) == 0:\n",
    "#         return list()\n",
    "#     t = variables[0]\n",
    "#     return [[float(x) for x in term.as_coeff_exponent(t)] for term in f.as_ordered_terms()]\n",
    "\n",
    "# def get_expr_type(f):\n",
    "#     s = str(f)\n",
    "#     for function, idx in FUNCTIONS.items():\n",
    "#         if function in s:\n",
    "#             return idx\n",
    "#     try:\n",
    "#         coeffs = get_coefficients_and_exponents(f)\n",
    "#     except:\n",
    "#         print(\"Cannot get coefficients for\", s)\n",
    "#         return -1\n",
    "#     if len(coeffs) == 0:\n",
    "#         return -1\n",
    "#     max_power = int(coeffs[0][1])\n",
    "#     if max_power > MAX_POWER:\n",
    "#         return -1\n",
    "#     return max_power\n",
    "\n",
    "# random.shuffle(originals)\n",
    "\n",
    "# exprs = []\n",
    "# labels = []\n",
    "# for i in range(len(originals)):\n",
    "#     f = sympify(originals[i])\n",
    "#     x, t = symbols(['x','t'])\n",
    "#     f = f.subs({t:x})\n",
    "#     #display(f)\n",
    "#     label = get_expr_type(f)\n",
    "#     if label < 0:\n",
    "#         print(\"Cannot process\", originals[i])\n",
    "#         continue\n",
    "#     exprs.append(f)\n",
    "#     labels.append(label)\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i, \"rows processed\")\n",
    "\n",
    "fin = open('datasets/random_exprs_1m.txt', 'r')\n",
    "lines = fin.readlines()\n",
    "exprs = []\n",
    "for line in lines[0:1000000]:\n",
    "    exprs.append(sympify(line))\n",
    "    if len(exprs) % 1000 == 0:\n",
    "        print(len(exprs), \"rows processed\")\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cd9302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/symbolic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to datasets...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class SampleEmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, exprs):\n",
    "        self.exprs = exprs\n",
    "        #self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {'inputs_embeds': torch.tensor(torch.rand(512,768)),\n",
    "        #         'attention_mask': torch.tensor(torch.ones(512)),\n",
    "        #        }\n",
    "        # item[\"labels\"] = torch.tensor([0.0, 1.0])\n",
    "        #print(str(self.exprs[idx]))\n",
    "        xs, ys = generate_sample(self.exprs[idx])\n",
    "        #print(\"ys\", ys)\n",
    "        encoding = sample2seq(ys, SEQ_LEN, EMBED_DIM, seq_first = True)\n",
    "        attention_mask = torch.cat([torch.tensor(encoding['attention_mask']), torch.ones(OUTPUT_LEN)])\n",
    "        inputs_embeds = torch.cat([torch.tensor(encoding['seq']), torch.zeros(OUTPUT_LEN, EMBED_DIM)])\n",
    "        item = {'inputs_embeds': inputs_embeds,\n",
    "                'attention_mask': attention_mask,\n",
    "                #'sample': sample\n",
    "               }\n",
    "        #scale = self.labels[idx]link\n",
    "        labels = tokenizer(str(self.exprs[idx]), return_tensors='pt', padding='max_length', truncation=True, max_length=OUTPUT_LEN)\n",
    "        input_ids = torch.cat([torch.ones(SEQ_LEN, dtype=torch.int64), labels['input_ids'][0]])\n",
    "        item[\"labels\"] = input_ids\n",
    "        item[\"inputs_ids\"] = input_ids.clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.exprs)\n",
    "\n",
    "    \n",
    "train_exprs = [exprs[i] for i in range(len(exprs)) if i % 50 != 0]\n",
    "test_exprs = [exprs[i] for i in range(len(exprs)) if i % 50 == 0]\n",
    "\n",
    "print(\"Converting to datasets...\")\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = SampleEmbeddingDataset(train_exprs)\n",
    "valid_dataset = SampleEmbeddingDataset(test_exprs)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4468a182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10513/1059701083.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.cat([torch.tensor(encoding['attention_mask']), torch.ones(OUTPUT_LEN)])\n",
      "/tmp/ipykernel_10513/1059701083.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_embeds = torch.cat([torch.tensor(encoding['seq']), torch.zeros(OUTPUT_LEN, EMBED_DIM)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>5*t**4 - 1.4*t</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(valid_dataset[1]['labels'][768:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c338e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaForCausalLM, RobertaConfig\n",
    "\n",
    "config = RobertaConfig(max_position_embeddings=SEQ_LEN+OUTPUT_LEN+2, hidden_size=EMBED_DIM, intermediate_size=EMBED_DIM*3, num_hidden_layers=6)\n",
    "config.is_decoder = True\n",
    "config.architectures = ['RobertaForCausalLM']\n",
    "\n",
    "#roberta_model = RobertaModel.from_pretrained(\"roberta-base\", problem_type='regression')\n",
    "roberta_model = RobertaForCausalLM(config)\n",
    "\n",
    "my_input = torch.rand(2,SEQ_LEN+OUTPUT_LEN,EMBED_DIM)\n",
    "\n",
    "outputs = roberta_model(inputs_embeds=my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0b3115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"architectures\": [\n",
       "    \"RobertaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 384,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1152,\n",
       "  \"is_decoder\": true,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "RobertaConfig {\n",
    "  \"_name_or_path\": \"roberta-base\",\n",
    "  \"architectures\": [\n",
    "    \"RobertaForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.36.2\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 50265\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7103b3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 50265])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fee118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import collections.abc\n",
    "import math\n",
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput, CausalLMOutputWithCrossAttentions, MaskedLMOutput, BaseModelOutputWithPoolingAndCrossAttentions\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "class TransformerWithEmbeddingInput(nn.Module):\n",
    "    def __init__(self, transformer_model, num_output=1) -> None:\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None\n",
    "    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n",
    "        #print(\"labels:\", labels, labels.shape)\n",
    "        embedding_output = self.transformer.roberta.embeddings(\n",
    "            input_ids=inputs_ids\n",
    "        )\n",
    "        #print(\"input 769\", inputs_ids[:,769])\n",
    "        #print(\"pos 769\", embedding_output[:,769,:])\n",
    "        sum_embedding = embedding_output + inputs_embeds\n",
    "#         print(\"inputs_embeds.shape\", inputs_embeds.shape)\n",
    "#         print(inputs_ids[:,0])\n",
    "#         print(inputs_ids[:,770])\n",
    "#         print(inputs_ids[:,-1])\n",
    "#         print(\"embedding_output.shape\", embedding_output.shape)\n",
    "#         print(\"pos 0\", embedding_output[:,0,:])\n",
    "#         print(\"pos 769\", embedding_output[:,770,:])\n",
    "#         print(\"pos -1\", embedding_output[:,-1,:])\n",
    "        \n",
    "        return self.transformer.forward(\n",
    "            #input_ids=inputs_ids,\n",
    "            inputs_embeds=sum_embedding,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "#         outputs = self.transformer(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "        \n",
    "#         last_hidden_states = outputs['last_hidden_state'][:, -1, :]\n",
    "#         logits = self.fc(last_hidden_states)\n",
    "        \n",
    "#         #class_label = class_label.to(logits.device)\n",
    "#         loss_fct = CrossEntropyLoss()\n",
    "#         loss = loss_fct(logits, labels)\n",
    "        \n",
    "#         return ImageClassifierOutput(\n",
    "#             loss=loss,\n",
    "#             logits=logits,\n",
    "#             hidden_states=None,\n",
    "#             attentions=None,\n",
    "#         )\n",
    "\n",
    "    \n",
    "wrapper_model = TransformerWithEmbeddingInput(roberta_model)\n",
    "#wrapper_model = torch.load('datasets/function_type_lm_test.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc06f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_10513/1059701083.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.cat([torch.tensor(encoding['attention_mask']), torch.ones(OUTPUT_LEN)])\n",
      "/tmp/ipykernel_10513/1059701083.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_embeds = torch.cat([torch.tensor(encoding['seq']), torch.zeros(OUTPUT_LEN, EMBED_DIM)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='651173' max='1225000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 651173/1225000 56:33:21 < 49:50:18, 3.20 it/s, Epoch 5.32/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.005976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.006859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.005871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.006294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.007834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.006280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.007510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.005692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.006418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.006038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.006142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.005437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.005997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.006130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.006053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.006328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.005239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.006545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.005981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.006209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.006098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.005810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.005380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.005367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.005573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.005914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.005686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.005437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.006255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.005940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.004875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.005736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.006111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.005682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.005562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.005939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.006175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.005997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410000</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.005699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.005376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.005976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.005924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.006103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.006052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480000</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.005984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.006847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.005977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.005399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.006464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.006678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.007064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.006082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.006183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.006798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.006402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.007104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.006726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.006504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.006037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 43\u001b[0m\n\u001b[1;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     34\u001b[0m     wrapper_model,\n\u001b[1;32m     35\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#tokenizer=tokenizer,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer.py:1821\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1818\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1820\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1821\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1822\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/accelerate/data_loader.py:458\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 458\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mSampleEmbeddingDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# item = {'inputs_embeds': torch.tensor(torch.rand(512,768)),\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#         'attention_mask': torch.tensor(torch.ones(512)),\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#        }\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# item[\"labels\"] = torch.tensor([0.0, 1.0])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#print(str(self.exprs[idx]))\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     xs, ys \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#print(\"ys\", ys)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m sample2seq(ys, SEQ_LEN, EMBED_DIM, seq_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mgenerate_sample\u001b[0;34m(f, min_x, max_x)\u001b[0m\n\u001b[1;32m     20\u001b[0m fl \u001b[38;5;241m=\u001b[39m lambdify((t), f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m xs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(min_x, max_x, increment)\n\u001b[0;32m---> 22\u001b[0m ys \u001b[38;5;241m=\u001b[39m \u001b[43mfl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ys, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ys, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     24\u001b[0m     ys \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(\u001b[38;5;28mlen\u001b[39m(xs), \u001b[38;5;28mfloat\u001b[39m(ys))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # evaluation_strategy = \"epoch\",\n",
    "    # save_strategy = \"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    save_steps=10000000,\n",
    "    eval_steps=10000,\n",
    "    logging_steps=5000,\n",
    "    save_total_limit=4,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    weight_decay=0.01,\n",
    "    output_dir='datasets/function_type_lm',\n",
    "    metric_for_best_model='accuracy')\n",
    "\n",
    "# Tie the weights of the embedding layer and the decoder layer\n",
    "# if hasattr(wrapper_model, 'tie_weights'):\n",
    "#     wrapper_model.tie_weights()\n",
    "\n",
    "trainer = Trainer(\n",
    "    wrapper_model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    #tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# train the model\n",
    "trainer.train() #'datasets/function_type_classifier/checkpoint-15000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8929bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10513/1059701083.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.cat([torch.tensor(encoding['attention_mask']), torch.ones(OUTPUT_LEN)])\n",
      "/tmp/ipykernel_10513/1059701083.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_embeds = torch.cat([torch.tensor(encoding['seq']), torch.zeros(OUTPUT_LEN, EMBED_DIM)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids: tensor([   0,  134,  111,  132,    4,  406, 3226,   90,    2,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1]) <s>1 - 2.7*t</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "768 -\n",
      "769 2\n",
      "770 .\n",
      "771 9\n",
      "772 *\n",
      "773 t\n",
      "774 </s>\n",
      "775 <pad>\n",
      "776 <pad>\n",
      "777 <pad>\n",
      "778 <pad>\n",
      "779 <pad>\n",
      "780 <pad>\n",
      "781 <pad>\n",
      "782 <pad>\n",
      "783 <pad>\n",
      "784 <pad>\n",
      "785 <pad>\n",
      "786 <pad>\n",
      "787 <pad>\n"
     ]
    }
   ],
   "source": [
    "wrapper_model.eval()\n",
    "example = valid_dataset[14]\n",
    "\n",
    "print(\"inputs_ids:\", example['inputs_ids'][768:790], tokenizer.decode(example['inputs_ids'][768:790]))\n",
    "\n",
    "BOS = 0\n",
    "PAD = 1\n",
    "EOS = 2\n",
    "START_IDX = 768\n",
    "\n",
    "inputs_ids = torch.ones(example['inputs_ids'].shape, dtype=example['inputs_ids'].dtype)\n",
    "inputs_ids[START_IDX] = BOS\n",
    "#inputs_ids[START_IDX+1] = 12\n",
    "\n",
    "\n",
    "idx = START_IDX\n",
    "\n",
    "while idx < SEQ_LEN + 20:\n",
    "    #print(\"input_ids:\", tokenizer.decode(inputs_ids[768:785]))\n",
    "    #print(\"inputs_embeds:\", example['inputs_embeds'][758:770])\n",
    "    #print(\"attention_mask:\", example['attention_mask'][768:780])\n",
    "    #print(\"labels:\", tokenizer.decode(example['labels'][768:780]))\n",
    "    outputs = wrapper_model(torch.unsqueeze(inputs_ids, 0).to('cuda:0'),\n",
    "                            torch.unsqueeze(example['inputs_embeds'], 0).to('cuda:0'), \n",
    "                            torch.unsqueeze(example['attention_mask'], 0).to('cuda:0'),\n",
    "                            torch.unsqueeze(example['labels'], 0).to('cuda:0'))\n",
    "    #\n",
    "    logits = outputs.logits\n",
    "    #print(\"logits:\", logits[:, idx, :])\n",
    "    predicted_token_id = torch.argmax(logits[:, idx, :], dim=-1)\n",
    "    # Convert the token ID to the actual token\n",
    "    predicted_token = tokenizer.decode(predicted_token_id)\n",
    "    print(idx, predicted_token)\n",
    "    idx += 1\n",
    "    #if idx > START_IDX+2:\n",
    "    inputs_ids[idx] = predicted_token_id.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "04f9cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ids = torch.ones(example['inputs_ids'].shape, dtype=example['inputs_ids'].dtype)\n",
    "inputs_ids[769] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "833debf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   288,     4,   406,  3226, 16254,  1640,   306,     4,   398,\n",
       "         3226,    90,    43,     2,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels'][768:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a744da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28235/3500901125.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.cat([torch.tensor(encoding['attention_mask']), torch.ones(OUTPUT_LEN)])\n",
      "/tmp/ipykernel_28235/3500901125.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_embeds = torch.cat([torch.tensor(encoding['seq']), torch.zeros(OUTPUT_LEN, EMBED_DIM)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><s>34*x**2 + 95*x + 66</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = valid_dataset[1000]\n",
    "tokenizer.decode(example['inputs_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aa43a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(wrapper_model, 'datasets/function_type_lm_800k_loss0.006.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbolic",
   "language": "python",
   "name": "symbolic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
