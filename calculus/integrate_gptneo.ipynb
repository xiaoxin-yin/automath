{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8aab416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data to infer the rules for integral\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sympy import sympify, lambdify, symbols, integrate, Interval, Symbol, I, S, oo, plot, evalf, N\n",
    "from IPython.display import display\n",
    "from utils.utils import *\n",
    "\n",
    "\n",
    "# def remove_constants(f):\n",
    "#     t = Symbol('t')\n",
    "#     return f.as_independent(t)[1]\n",
    "\n",
    "# fin = open(\"datasets/parametric_equations_polynomial_integral_results.json\", \"r\")\n",
    "# lines = fin.readlines()\n",
    "# fin.close()\n",
    "# fin = open(\"datasets/parametric_equations_randomized_polynomial_integral_results.json\", \"r\")\n",
    "# lines.extend(fin.readlines())\n",
    "# fin.close()\n",
    "\n",
    "# MAX_POWER = 6\n",
    "# MAX_AVG_DIFF = 0.01\n",
    "\n",
    "# originals = []\n",
    "# integrals = []\n",
    "# t = Symbol('t')\n",
    "\n",
    "# for line in lines:\n",
    "#     result = json.loads(line)\n",
    "#     if \"rounded_regressed\" not in result:\n",
    "#         continue\n",
    "#     original = round_all_floats(N(sympify(result[\"original\"])))\n",
    "#     integral = remove_constants(round_all_floats(N(sympify(result[\"rounded_regressed\"]))))\n",
    "#     try:\n",
    "#         original = filter_non_polynomial(original)\n",
    "#         integral = filter_non_polynomial(integral)\n",
    "#         original_integral = integrate(original, t)\n",
    "#         avg_diff = get_avg_diff(original_integral, integral, t)\n",
    "#         if avg_diff > MAX_AVG_DIFF or len(original_integral.args) != len(integral.args):\n",
    "#             print(\"Skipping. Diff=\", avg_diff)\n",
    "#             display(original_integral)\n",
    "#             display(integral)\n",
    "#             continue\n",
    "#     except:\n",
    "#         print(\"Cannot filter non-polynomials on\", str(integral))\n",
    "#         continue\n",
    "#     originals.append(str(original))\n",
    "#     integrals.append(str(integral))\n",
    "#     if len(originals) % 100 == 0:\n",
    "#         print(len(originals), \"cases loaded\")\n",
    "    \n",
    "# fin.close()\n",
    "\n",
    "# sentences = []\n",
    "# for i in range(len(originals)):\n",
    "#     sentences.append(originals[i] + ' entail ' + integrals[i] + ' end')\n",
    "# fin.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12f7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/symbolic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "# random.shuffle(sentences)\n",
    "\n",
    "# ds = Dataset.from_dict({'eq_pair': sentences})\n",
    "# train_ds = ds.train_test_split(test_size=0.025)\n",
    "\n",
    "# train_ds['train'][1]\n",
    "\n",
    "# train_ds.save_to_disk(\"datasets/integrate_gptneo_dataset_50k\")\n",
    "\n",
    "train_ds = load_from_disk(\"datasets/integrate_gptneo_dataset_50k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad73e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['eq_pair'],\n",
       "        num_rows: 47881\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['eq_pair'],\n",
       "        num_rows: 1228\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de73e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "CONTEXT_LENGTH = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xhyi/PT_GPTNEO350_ATG\") #\"EleutherAI/gpt-neo-125m\") \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"eq_pair\"], padding='max_length', truncation=True, max_length=CONTEXT_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_ds = train_ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=train_ds[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "lm_dataset = tokenized_ds.map(preprocess_function, batched=True, num_proc=1)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a35f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(lm_dataset['train'])):\n",
    "#     if len(lm_dataset['train'][i]['input_ids']) != 256:\n",
    "#         print(i, len(lm_dataset['train'][i]['input_ids']))\n",
    "#     if len(lm_dataset['train'][i]['labels']) != 256:\n",
    "#         print(i, len(lm_dataset['train'][i]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09682ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"xhyi/PT_GPTNEO350_ATG\")  #\"EleutherAI/gpt-neo-125m\")\n",
    "# config = model0.config\n",
    "\n",
    "# model = AutoModelForCausalLM.from_config(config)\n",
    "# model0 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='179' max='14965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  179/14965 01:09 < 1:36:57, 2.54 it/s, Epoch 0.06/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"datasets/integrate_gptneo_350m_50k_202401\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    save_total_limit=4,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e01fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"datasets/integrate_gptneo_202401/gptneo-350m-14000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e06cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"datasets/integrate_gptneo_202401/gptneo-350m-5500-loss0.350.model\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def generate_integral(input):\n",
    "    # Encode some input text\n",
    "    prompt = input + \" entail\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    # Generate text\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0.7)\n",
    "    # Decode and print the output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    #print(generated_text)\n",
    "    if 'entail' in generated_text:\n",
    "        generated_text = generated_text[generated_text.find('entail') + 6:].strip()\n",
    "        #print(generated_text)\n",
    "    if 'end' in generated_text:\n",
    "        generated_text = generated_text[0:generated_text.find('end')].strip()\n",
    "    return generated_text\n",
    "    \n",
    "print(generate_integral(\"2.5*t**3 + 0.51*t**2 + 3.2*t + 1.2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed15bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from utils.utils import *\n",
    "\n",
    "MIN_ALLOWED_DIFF = 0.011\n",
    "\n",
    "test_ds = train_ds['test']\n",
    "\n",
    "num_processed = 0\n",
    "num_equal = 0\n",
    "num_zero_diff = 0\n",
    "num_within_allowed = 0\n",
    "t = sp.Symbol('t')\n",
    "\n",
    "verbose = False\n",
    "\n",
    "for i in range(min(len(test_ds), 1000)):\n",
    "    if verbose:\n",
    "        print(\"Case\", i, test_ds[i]['eq_pair'])\n",
    "    eq_pair = test_ds[i]['eq_pair']\n",
    "    question = eq_pair[0:eq_pair.find('entail')].strip()\n",
    "    answer = eq_pair[eq_pair.find('entail') + 6:].replace('end', '').strip()\n",
    "    if 'repeat' in question:\n",
    "        question = question[0:question.find('repeat')]\n",
    "    original = sp.sympify(question)\n",
    "    integral = round_all_floats(sp.integrate(original), 2)\n",
    "    if verbose: display(integral)\n",
    "    try:\n",
    "        pred = generate_integral(question)\n",
    "        generated = round_all_floats(sp.sympify(pred), 2)\n",
    "        if verbose: display(generated)\n",
    "    except:\n",
    "        print(\"Cannot sympify\", pred)\n",
    "        continue\n",
    "    avg_diff = get_avg_diff(integral, generated, t)\n",
    "    if verbose: print(\"avg_diff\", avg_diff)\n",
    "    num_processed += 1\n",
    "    if avg_diff <= MIN_ALLOWED_DIFF:\n",
    "        num_within_allowed += 1\n",
    "    else:\n",
    "        display(integral)\n",
    "        display(generated)\n",
    "        print(avg_diff)\n",
    "    if avg_diff <= 0.0000001:\n",
    "        num_zero_diff += 1\n",
    "    diff_expr = sp.simplify(integral-generated)\n",
    "    if verbose: display(diff_expr)\n",
    "    if diff_expr == 0:\n",
    "        if verbose: print(\"Equal\")\n",
    "        num_equal += 1\n",
    "    if i%100 == 0:\n",
    "        print(i+1, \"processed\")\n",
    "        \n",
    "print(\"num_processed\", num_processed)\n",
    "print(\"num_equal\", num_equal)\n",
    "print(\"num_zero_diff\", num_zero_diff)\n",
    "print(\"num_within_allowed\", num_within_allowed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:symbolic]",
   "language": "python",
   "name": "conda-env-symbolic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
