{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8aab416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/symbolic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 960\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "data = json.load(open(\"/home/mcwave/code/test/eqs_flan_template.json\"))\n",
    "questions = []\n",
    "answers = []\n",
    "templates = []\n",
    "it = 0\n",
    "for eq in data:\n",
    "    questions.append(eq['question'])\n",
    "    expr = eq[\"answer\"]\n",
    "    new_expr = expr\n",
    "    found = False\n",
    "    for i in range(len(expr) - 1):\n",
    "        if expr[i:i + 2] == \"e-\":\n",
    "            found = True\n",
    "            new_expr = new_expr.replace(expr[i - 5:i + 3], \"0\")\n",
    "    answers.append(new_expr)\n",
    "    templates.append(eq[\"template\"])\n",
    "    it += 1\n",
    "    if it == 1000:\n",
    "        break\n",
    "random.shuffle(answers)\n",
    "ds = Dataset.from_dict({'question': questions, 'answer': answers})\n",
    "train_ds = ds.train_test_split(test_size=0.04)\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ec5d72-8078-46b7-babb-8e77b64e093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197376])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import time\n",
    "import math\n",
    "import sympy as sp\n",
    "from math import sin, cos, tan, sinh, cosh, tanh, asin, acos, atan, sqrt, log\n",
    "import time\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "SEQ_LEN = 128\n",
    "RANGE_START = 0.001\n",
    "RANGE_END = 4\n",
    "EMBED_SIZE = 768\n",
    "LOWER_BOUND = -10\n",
    "UPPER_BOUND = 10\n",
    "\n",
    "# Input:\n",
    "#    formula: A formula containing \"x\", which will be replaced to numbers between range_start and range_end\n",
    "# Output:\n",
    "#    a sequence of embeddings, each has embed_size dimensions, and each dimension is between LOWER_BOUND and UPPER_BOUND\n",
    "total_points = SEQ_LEN * EMBED_SIZE\n",
    "step = (RANGE_END - RANGE_START) / (total_points - 1)\n",
    "x = sp.Symbol(\"x\")\n",
    "\n",
    "def generate_seq_embed(expr):\n",
    "    f = sp.lambdify(x, expr, \"numpy\")\n",
    "    nums = np.arange(RANGE_START, RANGE_END + step, step)\n",
    "    # print(f(nums))\n",
    "    # res = list(np.concatenate((f(nums), np.zeros(129 * 768)), axis = 0))\n",
    "    res = f(nums)\n",
    "    if np.iscomplexobj(res):\n",
    "        return 1/0\n",
    "    res = np.concatenate((res, np.zeros(129 * 768)), axis = 0)\n",
    "    return res\n",
    "    # x = RANGE_START\n",
    "    # seq = [0] * (SEQ_LEN + 129) * 768\n",
    "    # seq[0] = expr(RANGE_START)\n",
    "    # for i in range(1, total_points):\n",
    "    #     try:\n",
    "    #         y = expr(x)\n",
    "    #         seq[i] = max(LOWER_BOUND, min(UPPER_BOUND, y))\n",
    "    #         # seq[i] = next(gen)\n",
    "    #     except:\n",
    "    #         seq[i] = seq[i - 1]\n",
    "    #     x += step\n",
    "    # return seq\n",
    "\n",
    "# exprs = []\n",
    "# embeds = []\n",
    "# i = 0\n",
    "# start_time = time.time()\n",
    "# ds = Dataset.from_dict({'question': [], \"answer\": [], \"inputs_embeds\": []})\n",
    "# empty = Dataset.from_dict({'question': [], \"answer\": [], \"inputs_embeds\": []})\n",
    "# temp = empty\n",
    "# for expr in answers:\n",
    "    \n",
    "#     # print(expr)\n",
    "#     if psutil.virtual_memory().percent > 75.0:\n",
    "#         break\n",
    "#     try:\n",
    "#         values = generate_seq_embed(expr)\n",
    "#         temp = temp.add_item({\"question\": \"[BOS]\", \"answer\": templates[i], \"inputs_embeds\": values})\n",
    "#     except:\n",
    "#         pass\n",
    "#     i += 1\n",
    "#     if i % 100 == 0:\n",
    "#         ds = datasets.concatenate_datasets([ds, temp], axis=0)\n",
    "#         temp = empty\n",
    "#         print(ds)\n",
    "#     if i % 500 == 0:\n",
    "#         print(i, time.time() - start_time)\n",
    "#         start_time = time.time()\n",
    "\n",
    "torch.Tensor(generate_seq_embed(\"sin(x)\")).shape\n",
    "# # i = 0\n",
    "# # def add_embeds(expr):\n",
    "# #     print(1234)\n",
    "# #     expr[\"inputs_embeds\"] = embeds[i]\n",
    "# #     i += 1\n",
    "# #     return expr\n",
    "# # print(\"map\")\n",
    "# ds = ds.add_column(\"inputs_embeds\", embeds)\n",
    "\n",
    "# train_ds = ds.train_test_split(test_size=0.04)\n",
    "# train_ds[\"train\"].save_to_disk(\"train_embeds_templates:1000.hf\")\n",
    "# print(\"Saved train\")\n",
    "# train_ds[\"test\"].save_to_disk(\"test_embeds_templates:1000.hf\")\n",
    "# print(\"Saved test\")\n",
    "# train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954b6fcf-af57-4efd-9856-fdb01e377fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_ds[\"train\"] = datasets.load_from_disk(\"train_embeds_templates:100000.hf\")\n",
    "# train_ds[\"test\"] = datasets.load_from_disk(\"test_embeds_templates:100000.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de73e66d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "# MODEL_NAME = \"results/2/checkpoint-1000\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "if tokenizer.bos_token is None:\n",
    "    tokenizer.add_special_tokens({'bos_token': '[BOS]'})\n",
    "\n",
    "CONTEXT_LENGTH = 128\n",
    "\n",
    "# We prefix our tasks with \"answer the question\"\n",
    "prefix = \"\"\n",
    "\n",
    "# Define the preprocessing function\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "    # The \"inputs\" are the tokenized answer:\n",
    "    inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=CONTEXT_LENGTH, truncation=True)\n",
    "  \n",
    "    # The \"labels\" are the tokenized outputs:\n",
    "    labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                        max_length=CONTEXT_LENGTH,         \n",
    "                        truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def add_padding(val):\n",
    "    val[\"labels\"] = val[\"labels\"][1:]\n",
    "    start_pad_tokens = [0] * 128\n",
    "    end_pad_tokens_ids = [0] * (129 - len(val[\"input_ids\"]))\n",
    "    end_pad_tokens_labels = [0] * (256 - len(val[\"labels\"]))\n",
    "    val[\"input_ids\"] = start_pad_tokens + val[\"input_ids\"] + end_pad_tokens_ids\n",
    "    val[\"labels\"] = [32100] + val[\"labels\"] + end_pad_tokens_labels\n",
    "    val[\"attention_mask\"] = [1] * 128 + val[\"attention_mask\"] + end_pad_tokens_ids\n",
    "    val[\"inputs_embeds\"] = list(val[\"inputs_embeds\"])\n",
    "    return val\n",
    "\n",
    "tokenized_dataset = train_ds#.map(preprocess_function, batched=True, remove_columns = [\"question\", \"answer\"], num_proc = 4).map(add_padding)\n",
    "# tokenized_dataset[\"train\"].save_to_disk(\"train_dataset_templates:100000.hf\")\n",
    "# print(\"Saved train\")\n",
    "# tokenized_dataset[\"test\"].save_to_disk(\"test_dataset_templates:100000.hf\")\n",
    "# print(\"Saved test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b60e8ed-a969-4a30-88e9-4cff4fe6a0ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# t = tokenized_dataset[\"test\"]\n",
    "# mask = t[\"attention_mask\"]\n",
    "# ids = t[\"input_ids\"]\n",
    "# labels = t[\"labels\"]\n",
    "# for i in range(len(t)):\n",
    "#     if len(mask[i]) != 257:\n",
    "#         print(\"mask\", i)\n",
    "#     if len(ids[i]) != 257:\n",
    "#         print(\"ids\", i)\n",
    "#     if len(labels[i]) != 257:\n",
    "#         print(\"labels\", i)\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff68a94b-c7ae-47a6-8cdc-c92bfc4d0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[\"train\"] = datasets.load_from_disk(\"train_dataset_templates:100000.hf\")\n",
    "tokenized_dataset[\"test\"] = datasets.load_from_disk(\"test_dataset_templates:100000.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a1d921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-18.9315, -18.8119, -18.6968,  ...,  -8.5230,  -8.5192,  -8.5154],\n",
       "        [ -8.5116,  -8.5079,  -8.5041,  ...,  -6.4849,  -6.4829,  -6.4810],\n",
       "        [ -6.4791,  -6.4772,  -6.4753,  ...,  -5.2824,  -5.2811,  -5.2798],\n",
       "        ...,\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(torch.Tensor(tokenized_dataset[\"train\"][102][\"inputs_embeds\"]), (257, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997d4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec tensor([[    0, 32100,    18,  ...,     0,     0,     0],\n",
      "        [    0, 32100,   152,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0, 32100,   599,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "seq tensor([[[-0.1322, -0.1135, -0.1459,  ...,  0.0469,  0.0588,  0.1472],\n",
      "         [-0.0760, -0.1236, -0.0156,  ...,  0.0734,  0.0972,  0.2680],\n",
      "         [ 0.0796, -0.0000, -0.0631,  ...,  0.0492,  0.1160,  0.1494],\n",
      "         ...,\n",
      "         [-0.0401,  0.0226,  0.0045,  ...,  0.2173, -0.0160,  0.0018],\n",
      "         [-0.0955,  0.0078, -0.0160,  ...,  0.2408,  0.0333,  0.0198],\n",
      "         [-0.1105, -0.0449, -0.0209,  ...,  0.0948,  0.0643,  0.1183]],\n",
      "\n",
      "        [[-0.2503, -0.1373, -0.3209,  ...,  0.0000, -0.0678,  0.0000],\n",
      "         [-0.2102,  0.0425, -0.3731,  ..., -0.0010, -0.0160,  0.0718],\n",
      "         [-0.1555, -0.0150, -0.1480,  ..., -0.0087,  0.1128,  0.0312],\n",
      "         ...,\n",
      "         [-0.0875, -0.1505, -0.1096,  ...,  0.0000,  0.2775,  0.0316],\n",
      "         [-0.1240, -0.0897, -0.0770,  ...,  0.4724,  0.1563,  0.0580],\n",
      "         [-0.0133, -0.1083, -0.2568,  ...,  0.5686,  0.2375, -0.1328]],\n",
      "\n",
      "        [[-0.0196, -0.0000,  0.0701,  ...,  0.0825,  0.0382,  0.0544],\n",
      "         [-0.1612, -0.0893,  0.2474,  ..., -0.0278,  0.0442, -0.0000],\n",
      "         [ 0.0320, -0.0971,  0.2338,  ...,  0.0794, -0.0000, -0.0479],\n",
      "         ...,\n",
      "         [ 0.0717,  0.0728,  0.3442,  ...,  0.1711, -0.2651,  0.1851],\n",
      "         [-0.1071,  0.0000,  0.2360,  ...,  0.1351, -0.0926,  0.1736],\n",
      "         [-0.3468,  0.1709,  0.2766,  ...,  0.3035, -0.1392,  0.0733]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2838, -0.2329, -0.0103,  ...,  0.3821,  0.2856, -0.0122],\n",
      "         [-0.1979, -0.2119, -0.0755,  ...,  0.5227,  0.0000,  0.1139],\n",
      "         [-0.2540, -0.3861, -0.1147,  ...,  0.4103,  0.2116,  0.0000],\n",
      "         ...,\n",
      "         [-0.0540, -0.0000,  0.0349,  ...,  0.4395,  0.1802,  0.1616],\n",
      "         [-0.0813, -0.1760, -0.0663,  ...,  0.4743,  0.1417,  0.1975],\n",
      "         [-0.1200, -0.1150, -0.0264,  ...,  0.3276,  0.1379,  0.1781]],\n",
      "\n",
      "        [[-0.2140, -0.2611, -0.1413,  ...,  0.0298, -0.1737,  0.0719],\n",
      "         [ 0.0593, -0.0302, -0.1486,  ...,  0.2864, -0.0514, -0.1120],\n",
      "         [ 0.0729,  0.0255, -0.0860,  ...,  0.2682, -0.0717, -0.0642],\n",
      "         ...,\n",
      "         [ 0.3763, -0.0348,  0.0634,  ...,  0.0624, -0.0557, -0.0101],\n",
      "         [ 0.2097, -0.0690,  0.0795,  ...,  0.1168, -0.0999,  0.0859],\n",
      "         [ 0.2123, -0.0680,  0.0640,  ...,  0.0419,  0.0127,  0.0272]],\n",
      "\n",
      "        [[-0.0536,  0.0081, -0.0754,  ...,  0.0515,  0.1346, -0.0495],\n",
      "         [-0.0228, -0.3064, -0.2500,  ...,  0.3322,  0.0295, -0.1289],\n",
      "         [-0.0684, -0.1515, -0.1747,  ...,  0.4747,  0.1594,  0.0000],\n",
      "         ...,\n",
      "         [-0.0769, -0.2278, -0.0573,  ...,  0.3436,  0.2962, -0.0053],\n",
      "         [-0.0451, -0.0910,  0.0706,  ...,  0.5822,  0.1682, -0.0483],\n",
      "         [-0.1023, -0.1946, -0.1998,  ...,  0.6244,  0.2858,  0.0459]]],\n",
      "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n",
      "logits tensor([[[ -59.5973,   -3.3128,  -11.9907,  ...,  -59.4312,  -59.7170,\n",
      "           -59.8376],\n",
      "         [ -59.8032,   -7.6282,  -11.5443,  ...,  -59.7740,  -59.9183,\n",
      "           -59.9527],\n",
      "         [ -61.7068,   -7.4110,  -16.2336,  ...,  -61.6548,  -61.8700,\n",
      "           -61.7951],\n",
      "         ...,\n",
      "         [ -46.6359,   -0.9583,   -7.6645,  ...,  -46.5214,  -46.9021,\n",
      "           -46.8857],\n",
      "         [ -42.8305,   -0.8468,   -6.7394,  ...,  -42.5140,  -43.1332,\n",
      "           -42.9311],\n",
      "         [ -41.6421,    0.9518,   -5.8003,  ...,  -41.7369,  -41.9064,\n",
      "           -42.0814]],\n",
      "\n",
      "        [[ -83.7833,  -12.9293,  -19.1537,  ...,  -83.3681,  -83.9809,\n",
      "           -83.8119],\n",
      "         [ -73.2833,   -8.3628,  -15.4807,  ...,  -72.7652,  -73.6956,\n",
      "           -73.0917],\n",
      "         [ -75.0173,  -11.7470,  -15.7097,  ...,  -74.6915,  -75.4099,\n",
      "           -75.1312],\n",
      "         ...,\n",
      "         [ -79.4555,  -16.4589,  -18.6803,  ...,  -79.1819,  -79.9004,\n",
      "           -79.8062],\n",
      "         [ -79.5005,  -13.2122,  -14.9398,  ...,  -79.0902,  -79.3372,\n",
      "           -79.4936],\n",
      "         [ -62.5589,   -9.8789,  -12.5109,  ...,  -61.7835,  -62.3894,\n",
      "           -62.2786]],\n",
      "\n",
      "        [[ -74.4835,   -7.7890,   -9.0313,  ...,  -74.3326,  -74.4647,\n",
      "           -74.8633],\n",
      "         [ -53.8747,   -5.1044,   -5.6838,  ...,  -53.6822,  -53.8424,\n",
      "           -54.2824],\n",
      "         [ -80.8372,  -10.5455,  -15.7099,  ...,  -80.7548,  -80.8852,\n",
      "           -81.0833],\n",
      "         ...,\n",
      "         [ -89.8549,  -17.6563,  -18.2010,  ...,  -89.1578,  -90.0662,\n",
      "           -90.0474],\n",
      "         [ -87.8936,  -13.1618,  -17.7977,  ...,  -87.4891,  -88.0983,\n",
      "           -88.0261],\n",
      "         [-100.1835,  -17.1101,  -20.3572,  ...,  -99.7781, -100.5404,\n",
      "           -99.8406]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -44.8590,   -2.5757,   -8.4794,  ...,  -44.4693,  -44.7795,\n",
      "           -44.8585],\n",
      "         [ -55.6196,   -6.2148,  -10.7368,  ...,  -55.3126,  -55.7205,\n",
      "           -55.7087],\n",
      "         [ -59.9430,   -7.7776,  -10.3698,  ...,  -59.6283,  -59.9995,\n",
      "           -60.1491],\n",
      "         ...,\n",
      "         [ -64.1867,   -7.6257,  -14.6347,  ...,  -63.9308,  -64.3853,\n",
      "           -64.4273],\n",
      "         [ -67.0691,   -6.9412,  -14.4936,  ...,  -66.9330,  -67.2755,\n",
      "           -67.3928],\n",
      "         [ -54.5905,   -6.2408,  -10.4285,  ...,  -54.4791,  -54.6430,\n",
      "           -54.8039]],\n",
      "\n",
      "        [[ -73.5067,  -12.5023,  -14.3400,  ...,  -73.0491,  -73.7585,\n",
      "           -73.7330],\n",
      "         [ -67.5108,   -5.8254,  -10.8352,  ...,  -66.9527,  -67.5993,\n",
      "           -67.7322],\n",
      "         [ -66.7422,   -9.3512,  -17.3561,  ...,  -66.1931,  -67.0556,\n",
      "           -66.6175],\n",
      "         ...,\n",
      "         [ -72.0862,   -6.8270,  -17.3308,  ...,  -71.5068,  -72.2280,\n",
      "           -72.3603],\n",
      "         [ -71.3153,   -4.9475,  -16.9400,  ...,  -70.8300,  -71.4937,\n",
      "           -71.3617],\n",
      "         [ -67.1646,  -10.0269,  -12.7819,  ...,  -66.7004,  -67.3185,\n",
      "           -67.2717]],\n",
      "\n",
      "        [[ -74.6985,  -12.6445,  -12.0478,  ...,  -74.1637,  -74.7724,\n",
      "           -74.5263],\n",
      "         [ -79.7918,  -12.5100,  -15.4852,  ...,  -79.6419,  -79.9974,\n",
      "           -80.0541],\n",
      "         [ -87.9903,  -17.8061,  -14.0347,  ...,  -87.6267,  -88.2150,\n",
      "           -88.1306],\n",
      "         ...,\n",
      "         [ -80.6964,  -13.6710,  -20.1691,  ...,  -80.2739,  -80.8291,\n",
      "           -80.8691],\n",
      "         [ -82.2069,  -13.1072,  -18.6951,  ...,  -81.8040,  -82.3852,\n",
      "           -82.3803],\n",
      "         [ -68.5208,   -9.4120,  -12.0536,  ...,  -68.1251,  -68.7103,\n",
      "           -68.6855]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "CrossEntropyLoss()\n",
      "tensor([[-59.5973,  -3.3128, -11.9907,  ..., -59.4312, -59.7170, -59.8376],\n",
      "        [-59.8032,  -7.6282, -11.5443,  ..., -59.7740, -59.9183, -59.9527],\n",
      "        [-61.7068,  -7.4110, -16.2336,  ..., -61.6548, -61.8700, -61.7951],\n",
      "        ...,\n",
      "        [-80.6964, -13.6710, -20.1691,  ..., -80.2739, -80.8291, -80.8691],\n",
      "        [-82.2069, -13.1072, -18.6951,  ..., -81.8040, -82.3852, -82.3803],\n",
      "        [-68.5208,  -9.4120, -12.0536,  ..., -68.1251, -68.7103, -68.6855]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor([32100,    18,     7,  ...,     0,     0,     0], device='cuda:0')\n",
      "tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/600 00:04 < 13:34, 0.73 it/s, Epoch 0.07/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec tensor([[    0, 32100,   599,  ...,     0,     0,     0],\n",
      "        [    0, 32100,    18,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     7,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0, 32100,     7,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        [    0, 32100,    17,  ...,     0,     0,     0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "seq tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeDropoutBackward0>)\n",
      "logits tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "CrossEntropyLoss()\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([32100,   599,   755,  ...,     0,     0,     0], device='cuda:0')\n",
      "tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "dec tensor([[    0, 32100,   599,  ...,     0,     0,     0],\n",
      "        [    0, 32100, 12989,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "seq tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeDropoutBackward0>)\n",
      "logits tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "CrossEntropyLoss()\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([32100,   599,   927,  ...,     0,     0,     0], device='cuda:0')\n",
      "tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec tensor([[    0, 32100,   599,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0, 32100,    17,  ...,     0,     0,     0],\n",
      "        [    0, 32100,   152,  ...,     0,     0,     0],\n",
      "        [    0, 32100,    52,  ...,     0,     0,     0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "seq tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeDropoutBackward0>)\n",
      "logits tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "CrossEntropyLoss()\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([32100,   599,   357,  ...,     0,     0,     0], device='cuda:0')\n",
      "tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "dec tensor([[    0, 32100,    18,  ...,     0,     0,     0],\n",
      "        [    0, 32100,    18,  ...,     0,     0,     0],\n",
      "        [    0, 32100,     9,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0, 32100,   599,  ...,     0,     0,     0],\n",
      "        [    0, 32100,   599,  ...,     0,     0,     0],\n",
      "        [    0, 32100,    18,  ...,     0,     0,     0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "seq "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return result\n",
    "\n",
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "PER_DEVICE_EVAL_BATCH = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 100\n",
    "NUM_EPOCHS = 10\n",
    "SAVE_STEPS=100\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"results/2/\",\n",
    "   evaluation_strategy=\"steps\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   save_steps=SAVE_STEPS,\n",
    "   eval_steps=SAVE_STEPS,\n",
    "   logging_steps=SAVE_STEPS,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f2e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode some input text \n",
    "generate_seq_embed(\"-sin(1.94*x - 1.94)\")\n",
    "inputs_embeds = torch.reshape(torch.Tensor(generate_seq_embed(\"-sin(1.94*x - 1.94)\")), [1, 257, 768]).to(\"cuda\")\n",
    "print(inputs_embeds)\n",
    "prompt = \"[BOS]\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")\n",
    "print(input_ids)\n",
    "# Generate text\n",
    "output = model.generate(input_ids = input_ids, labels = input_ids, inputs_embeds = inputs_embeds, max_new_tokens = 50, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "# Decode and print the output\n",
    "generated_text = tokenizer.decode(output[0])\n",
    "print(output, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fa48f-371e-4ad0-b4f4-fe5b5b8fcf39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([1,2,3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d99fc-3737-4734-99ca-c4b5ecdaa8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(torch.reshape(torch.Tensor(generate_seq_embed(\"0.123*x**2+0.016*x-0.707\")), [1, 257, 768])[:, :128].to(\"cuda\"))\n",
    "# print(torch.reshape(torch.Tensor(generate_seq_embed(eval(\"lambda x: 1.361*math.cos(2.0*x) - 0.167\"))), [1, 257, 768])[:, :128].to(\"cuda\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbolic",
   "language": "python",
   "name": "symbolic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
