{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8aab416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/symbolic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 960\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "data = json.load(open(\"/home/mcwave/code/test/eqs_flan_template.json\"))\n",
    "questions = []\n",
    "answers = []\n",
    "templates = []\n",
    "it = 0\n",
    "for eq in data:\n",
    "    questions.append(eq['question'])\n",
    "    expr = eq[\"answer\"]\n",
    "    new_expr = expr\n",
    "    found = False\n",
    "    for i in range(len(expr) - 1):\n",
    "        if expr[i:i + 2] == \"e-\":\n",
    "            found = True\n",
    "            new_expr = new_expr.replace(expr[i - 5:i + 3], \"0\")\n",
    "    answers.append(new_expr)\n",
    "    templates.append(eq[\"template\"])\n",
    "    it += 1\n",
    "    if it == 1000:\n",
    "        break\n",
    "random.shuffle(answers)\n",
    "ds = Dataset.from_dict({'question': questions, 'answer': answers})\n",
    "train_ds = ds.train_test_split(test_size=0.04)\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ec5d72-8078-46b7-babb-8e77b64e093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197376])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import time\n",
    "import math\n",
    "import sympy as sp\n",
    "from math import sin, cos, tan, sinh, cosh, tanh, asin, acos, atan, sqrt, log\n",
    "import time\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "SEQ_LEN = 128\n",
    "RANGE_START = 0.001\n",
    "RANGE_END = 4\n",
    "EMBED_SIZE = 768\n",
    "LOWER_BOUND = -10\n",
    "UPPER_BOUND = 10\n",
    "\n",
    "# Input:\n",
    "#    formula: A formula containing \"x\", which will be replaced to numbers between range_start and range_end\n",
    "# Output:\n",
    "#    a sequence of embeddings, each has embed_size dimensions, and each dimension is between LOWER_BOUND and UPPER_BOUND\n",
    "total_points = SEQ_LEN * EMBED_SIZE\n",
    "step = (RANGE_END - RANGE_START) / (total_points - 1)\n",
    "x = sp.Symbol(\"x\")\n",
    "\n",
    "def generate_seq_embed(expr):\n",
    "    f = sp.lambdify(x, expr, \"numpy\")\n",
    "    nums = np.arange(RANGE_START, RANGE_END + step, step)\n",
    "    # print(f(nums))\n",
    "    # res = list(np.concatenate((f(nums), np.zeros(129 * 768)), axis = 0))\n",
    "    res = f(nums)\n",
    "    if np.iscomplexobj(res):\n",
    "        return 1/0\n",
    "    res = np.concatenate((res, np.zeros(129 * 768)), axis = 0)\n",
    "    return res\n",
    "    # x = RANGE_START\n",
    "    # seq = [0] * (SEQ_LEN + 129) * 768\n",
    "    # seq[0] = expr(RANGE_START)\n",
    "    # for i in range(1, total_points):\n",
    "    #     try:\n",
    "    #         y = expr(x)\n",
    "    #         seq[i] = max(LOWER_BOUND, min(UPPER_BOUND, y))\n",
    "    #         # seq[i] = next(gen)\n",
    "    #     except:\n",
    "    #         seq[i] = seq[i - 1]\n",
    "    #     x += step\n",
    "    # return seq\n",
    "\n",
    "# exprs = []\n",
    "# embeds = []\n",
    "# i = 0\n",
    "# start_time = time.time()\n",
    "# ds = Dataset.from_dict({'question': [], \"answer\": [], \"inputs_embeds\": []})\n",
    "# empty = Dataset.from_dict({'question': [], \"answer\": [], \"inputs_embeds\": []})\n",
    "# temp = empty\n",
    "# for expr in answers:\n",
    "    \n",
    "#     # print(expr)\n",
    "#     if psutil.virtual_memory().percent > 75.0:\n",
    "#         break\n",
    "#     try:\n",
    "#         values = generate_seq_embed(expr)\n",
    "#         temp = temp.add_item({\"question\": \"[BOS]\", \"answer\": templates[i], \"inputs_embeds\": values})\n",
    "#     except:\n",
    "#         pass\n",
    "#     i += 1\n",
    "#     if i % 100 == 0:\n",
    "#         ds = datasets.concatenate_datasets([ds, temp], axis=0)\n",
    "#         temp = empty\n",
    "#         print(ds)\n",
    "#     if i % 500 == 0:\n",
    "#         print(i, time.time() - start_time)\n",
    "#         start_time = time.time()\n",
    "\n",
    "torch.Tensor(generate_seq_embed(\"sin(x)\")).shape\n",
    "# # i = 0\n",
    "# # def add_embeds(expr):\n",
    "# #     print(1234)\n",
    "# #     expr[\"inputs_embeds\"] = embeds[i]\n",
    "# #     i += 1\n",
    "# #     return expr\n",
    "# # print(\"map\")\n",
    "# ds = ds.add_column(\"inputs_embeds\", embeds)\n",
    "\n",
    "# train_ds = ds.train_test_split(test_size=0.04)\n",
    "# train_ds[\"train\"].save_to_disk(\"train_embeds_templates:1000.hf\")\n",
    "# print(\"Saved train\")\n",
    "# train_ds[\"test\"].save_to_disk(\"test_embeds_templates:1000.hf\")\n",
    "# print(\"Saved test\")\n",
    "# train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954b6fcf-af57-4efd-9856-fdb01e377fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_ds[\"train\"] = datasets.load_from_disk(\"train_embeds_templates:100000.hf\")\n",
    "# train_ds[\"test\"] = datasets.load_from_disk(\"test_embeds_templates:100000.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de73e66d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "# MODEL_NAME = \"results/2/checkpoint-1000\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "if tokenizer.bos_token is None:\n",
    "    tokenizer.add_special_tokens({'bos_token': '[BOS]'})\n",
    "\n",
    "CONTEXT_LENGTH = 128\n",
    "\n",
    "# We prefix our tasks with \"answer the question\"\n",
    "prefix = \"\"\n",
    "\n",
    "# Define the preprocessing function\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "    # The \"inputs\" are the tokenized answer:\n",
    "    inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=CONTEXT_LENGTH, truncation=True)\n",
    "  \n",
    "    # The \"labels\" are the tokenized outputs:\n",
    "    labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                        max_length=CONTEXT_LENGTH,         \n",
    "                        truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def add_padding(val):\n",
    "    val[\"labels\"] = val[\"labels\"][1:]\n",
    "    start_pad_tokens = [0] * 128\n",
    "    end_pad_tokens_ids = [0] * (129 - len(val[\"input_ids\"]))\n",
    "    end_pad_tokens_labels = [0] * (256 - len(val[\"labels\"]))\n",
    "    val[\"input_ids\"] = start_pad_tokens + val[\"input_ids\"] + end_pad_tokens_ids\n",
    "    val[\"labels\"] = [32100] + val[\"labels\"] + end_pad_tokens_labels\n",
    "    val[\"attention_mask\"] = [1] * 128 + val[\"attention_mask\"] + end_pad_tokens_ids\n",
    "    val[\"inputs_embeds\"] = list(val[\"inputs_embeds\"])\n",
    "    return val\n",
    "\n",
    "tokenized_dataset = train_ds#.map(preprocess_function, batched=True, remove_columns = [\"question\", \"answer\"], num_proc = 4).map(add_padding)\n",
    "# tokenized_dataset[\"train\"].save_to_disk(\"train_dataset_templates:100000.hf\")\n",
    "# print(\"Saved train\")\n",
    "# tokenized_dataset[\"test\"].save_to_disk(\"test_dataset_templates:100000.hf\")\n",
    "# print(\"Saved test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b60e8ed-a969-4a30-88e9-4cff4fe6a0ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# t = tokenized_dataset[\"test\"]\n",
    "# mask = t[\"attention_mask\"]\n",
    "# ids = t[\"input_ids\"]\n",
    "# labels = t[\"labels\"]\n",
    "# for i in range(len(t)):\n",
    "#     if len(mask[i]) != 257:\n",
    "#         print(\"mask\", i)\n",
    "#     if len(ids[i]) != 257:\n",
    "#         print(\"ids\", i)\n",
    "#     if len(labels[i]) != 257:\n",
    "#         print(\"labels\", i)\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff68a94b-c7ae-47a6-8cdc-c92bfc4d0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[\"train\"] = datasets.load_from_disk(\"train_dataset_templates:100000.hf\")\n",
    "tokenized_dataset[\"test\"] = datasets.load_from_disk(\"test_dataset_templates:100000.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997d4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.embedding\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101/600 02:02 < 10:19, 0.81 it/s, Epoch 1.67/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n",
      "nn.embedding\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m\n\u001b[1;32m     31\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m     32\u001b[0m    output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/2/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m    evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m    push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     48\u001b[0m    model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     49\u001b[0m    args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m    compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     55\u001b[0m )\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer.py:1914\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1912\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1914\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer.py:2263\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2261\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2263\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer.py:3012\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3009\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3011\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3012\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3015\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3016\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3020\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3022\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer.py:3201\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3198\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3200\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3201\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3202\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3203\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:296\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    292\u001b[0m ):\n\u001b[1;32m    293\u001b[0m     generation_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    294\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m     }\n\u001b[0;32m--> 296\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/generation/utils.py:1553\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m eos_token_id\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1553\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_model_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1556\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/generation/utils.py:658\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_model_inputs\u001b[0;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    659\u001b[0m     inputs, input_name \u001b[38;5;241m=\u001b[39m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# 4. if `inputs` is still None, try to create `input_ids` from BOS token\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: You passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return result\n",
    "\n",
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "PER_DEVICE_EVAL_BATCH = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 100\n",
    "NUM_EPOCHS = 10\n",
    "SAVE_STEPS=100\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"results/2/\",\n",
    "   evaluation_strategy=\"steps\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   save_steps=SAVE_STEPS,\n",
    "   eval_steps=SAVE_STEPS,\n",
    "   logging_steps=SAVE_STEPS,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b5c6e-6d98-4920-8134-55d88c8911f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tokens = torch.nn.Embedding(32168, 768).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8256b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.IntTensor([[32100, 1] + [0] * 127] * 16).to(\"cuda\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb98334",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2e216-ca06-44c4-923d-c098db43d72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f2e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode some input text \n",
    "generate_seq_embed(\"-sin(1.94*x - 1.94)\")\n",
    "inputs_embeds = torch.reshape(torch.Tensor(generate_seq_embed(\"-sin(1.94*x - 1.94)\")), [1, 257, 768]).to(\"cuda\")\n",
    "print(inputs_embeds)\n",
    "prompt = \"[BOS]\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")\n",
    "print(input_ids)\n",
    "# Generate text\n",
    "output = model.generate(input_ids = input_ids, labels = input_ids, inputs_embeds = inputs_embeds, max_new_tokens = 50, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "# Decode and print the output\n",
    "generated_text = tokenizer.decode(output[0])\n",
    "print(output, generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fa48f-371e-4ad0-b4f4-fe5b5b8fcf39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([1,2,3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d99fc-3737-4734-99ca-c4b5ecdaa8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(torch.reshape(torch.Tensor(generate_seq_embed(\"0.123*x**2+0.016*x-0.707\")), [1, 257, 768])[:, :128].to(\"cuda\"))\n",
    "# print(torch.reshape(torch.Tensor(generate_seq_embed(eval(\"lambda x: 1.361*math.cos(2.0*x) - 0.167\"))), [1, 257, 768])[:, :128].to(\"cuda\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbolic",
   "language": "python",
   "name": "symbolic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
