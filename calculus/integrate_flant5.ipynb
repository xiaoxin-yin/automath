{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12882a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data to infer the rules for integral\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sympy import sympify, lambdify, symbols, integrate, Interval, Symbol, I, S, oo, plot, evalf, N\n",
    "from IPython.display import display\n",
    "from utils.utils import *\n",
    "\n",
    "\n",
    "def remove_constants(f):\n",
    "    t = Symbol('t')\n",
    "    return f.as_independent(t)[1]\n",
    "\n",
    "fin = open(\"datasets/parametric_equations_polynomial_integral_results.json\", \"r\")\n",
    "lines = fin.readlines()\n",
    "fin.close()\n",
    "fin = open(\"datasets/parametric_equations_randomized_polynomial_integral_results.json\", \"r\")\n",
    "lines.extend(fin.readlines())\n",
    "fin.close()\n",
    "\n",
    "MAX_POWER = 6\n",
    "MAX_AVG_DIFF = 0.01\n",
    "\n",
    "originals = []\n",
    "integrals = []\n",
    "t = Symbol('t')\n",
    "\n",
    "for line in lines:\n",
    "    result = json.loads(line)\n",
    "    if \"rounded_regressed\" not in result:\n",
    "        continue\n",
    "    original = round_all_floats(N(sympify(result[\"original\"])))\n",
    "    integral = remove_constants(round_all_floats(N(sympify(result[\"rounded_regressed\"]))))\n",
    "    #try:\n",
    "    original = filter_non_polynomial(original)\n",
    "    integral = filter_non_polynomial(integral)\n",
    "    original_integral = integrate(original, t)\n",
    "    avg_diff = get_avg_diff(original_integral, integral, t)\n",
    "    if avg_diff > MAX_AVG_DIFF or len(original_integral.args) != len(integral.args):\n",
    "        print(\"Skipping. Diff=\", avg_diff)\n",
    "        display(original_integral)\n",
    "        display(integral)\n",
    "        continue\n",
    "#     except:\n",
    "#         print(\"Cannot filter non-polynomials on\", str(integral))\n",
    "#         continue\n",
    "    originals.append(str(original))\n",
    "    integrals.append(str(integral))\n",
    "    if len(originals) % 100 == 0:\n",
    "        print(len(originals), \"cases loaded\")\n",
    "    \n",
    "fin.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b553c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1.2 t^{5} + 0.98 t^{4} + 0.32 t^{3} + 0.52 t^{2} + 2.81 t$"
      ],
      "text/plain": [
       "1.2*t**5 + 0.98*t**4 + 0.32*t**3 + 0.52*t**2 + 2.81*t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = sp.sympify(\"1.2*t**5 + 0.98*t**4 + 0.32*t**3 + 0.52*t**2 + 2.81*t - 0.09\")\n",
    "t = Symbol('t')\n",
    "display(f.as_independent(t)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8aab416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/symbolic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 't**4 - 7.0*t**3 + 4.03*t**2 - 5.92*t + 1.63',\n",
       " 'answer': '0.2*t**5 - 1.75*t**4 + 1.34*t**3 - 2.98*t**2 + 1.6*t'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "# for i in range(len(originals)):\n",
    "#     originals[i] = originals[i] + ' repeat ' + originals[i]\n",
    "\n",
    "ds = Dataset.from_dict({'question': originals, 'answer':integrals})\n",
    "ds = ds.shuffle()\n",
    "train_ds = ds.train_test_split(test_size=0.05)\n",
    "\n",
    "train_ds['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12f7122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 19514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1028\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de73e66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19514/19514 [00:01<00:00, 18771.82 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1028/1028 [00:00<00:00, 20682.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-large\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "CONTEXT_LENGTH = 128\n",
    "\n",
    "# We prefix our tasks with \"answer the question\"\n",
    "prefix = \"\"\n",
    "\n",
    "# Define the preprocessing function\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "   # The \"inputs\" are the tokenized answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=CONTEXT_LENGTH, truncation=True)\n",
    "  \n",
    "   # The \"labels\" are the tokenized outputs:\n",
    "   labels = tokenizer(text_target=examples[\"answer\"], \n",
    "                      max_length=CONTEXT_LENGTH,         \n",
    "                      truncation=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs\n",
    "\n",
    "tokenized_dataset = train_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\") #\"EleutherAI/gpt-neo-125m\") \"xhyi/PT_GPTNEO350_ATG\"\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples[\"eq_pair\"], padding='max_length', truncation=True, max_length=CONTEXT_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "# tokenized_ds = train_ds.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=1,\n",
    "#     remove_columns=train_ds[\"train\"].column_names,\n",
    "# )\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "#     return examples\n",
    "\n",
    "# lm_dataset = tokenized_ds.map(preprocess_function, batched=True, num_proc=1)\n",
    "\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95e12e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '4.5*t - 8.5',\n",
       " 'answer': '2.25*t**2 - 8.5*t',\n",
       " 'input_ids': [3, 12451, 1935, 17, 3, 18, 3, 19253, 1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [1682, 1828, 1935, 17, 19844, 357, 3, 18, 3, 19253, 1935, 17, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f036ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1000\n",
      "7.57*t + 1.57\n",
      "3.79*t**2 + 1.57*t</s>\n",
      "Case 1001\n",
      "73.0*t**2 + 39.0*t + 5.33\n",
      "24.33*t**3 + 19.5*t**2 + 5.33*t</s>\n",
      "Case 1002\n",
      "7.25*t - 17.0\n",
      "3.62*t**2 - 17.0*t</s>\n",
      "Case 1003\n",
      "21.87*t**2 + 19.1*t + 10.16\n",
      "7.29*t**3 + 9.55*t**2 + 10.16*t</s>\n",
      "Case 1004\n",
      "6.5*t + 5.0\n",
      "3.25*t**2 + 5.0*t</s>\n",
      "Case 1005\n",
      "8.8*t - 7.4\n",
      "4.4*t**2 - 7.4*t</s>\n",
      "Case 1006\n",
      "11.25 - 4.0*t\n",
      "-2.0*t**2 + 11.25*t</s>\n",
      "Case 1007\n",
      "3.0*t**4 + 3.0*t**3 + 1.01*t**2 + 0.59*t + 5.08\n",
      "0.6*t**5 + 0.75*t**4 + 0.36*t**3 + 0.35*t**2 + 5.16*t</s>\n",
      "Case 1008\n",
      "3.2*t + 8.4\n",
      "1.6*t**2 + 8.4*t</s>\n",
      "Case 1009\n",
      "-4.2*t - 0.6\n",
      "-2.1*t**2 - 0.6*t</s>\n"
     ]
    }
   ],
   "source": [
    "tmp = tokenized_dataset['train']\n",
    "\n",
    "for i in range(1000, min(len(tmp), 1010)):\n",
    "    question = tmp[i]['question'] \n",
    "    if len(question) > 3:\n",
    "        print(\"Case\", i)\n",
    "        print(question)\n",
    "        print(tokenizer.decode(tmp[i]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "997d4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='12200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    5/12200 00:00 < 43:37, 4.66 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "   return result\n",
    "\n",
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 8\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 20\n",
    "NUM_EPOCHS = 5\n",
    "SAVE_STEPS=1000\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"datasets/integrate_flant5_large_20240101\",\n",
    "   evaluation_strategy=\"steps\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   save_steps=SAVE_STEPS,\n",
    "   eval_steps=SAVE_STEPS,\n",
    "   logging_steps=SAVE_STEPS,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "#cp_path = \"datasets/integrate_flant5_20240101/checkpoint-36000\"\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4e01fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"datasets/integrate_flant5_20240101/flant5-large-11000-loss0.050.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e06cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\"datasets/normalize_symbolic_regression_results_20231219/gptneo-350m-22000-loss0.443.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9f2e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m        answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_integral\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2.5*t**3 + 0.51*t**2 + 68.55\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mgenerate_integral\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text_target\u001b[38;5;241m=\u001b[39minputs, \n\u001b[1;32m      8\u001b[0m                    max_length\u001b[38;5;241m=\u001b[39mCONTEXT_LENGTH,         \n\u001b[1;32m      9\u001b[0m                    truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m                    return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(inputs)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     14\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/generation/utils.py:1593\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1586\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1588\u001b[0m         )\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1593\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/generation/utils.py:742\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    740\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    741\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 742\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1016\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1016\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# required mask seq length can be calculated via length of past\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symbolic/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "#inputs = tokenizer(inputs, return_tensors=\"pt\").to(device)\n",
    "\n",
    "def generate_integral(inputs):\n",
    "    inputs = tokenizer(text_target=inputs, \n",
    "                       max_length=CONTEXT_LENGTH,         \n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    #print(inputs)\n",
    "    outputs = model.generate(**inputs, temperature=0.01)\n",
    "    answer = tokenizer.decode(outputs[0])\n",
    "    answer = answer.replace('<pad>','').replace('</s>','').strip()\n",
    "    if answer[-1]=='*':\n",
    "       answer = answer+'t'\n",
    "    return answer\n",
    "\n",
    "print(generate_integral(\"2.5*t**3 + 0.51*t**2 + 68.55\"))\n",
    "\n",
    "# # Encode some input text\n",
    "# prompt = \"0.33*t**3 - 1.0*t**2 entail\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# # Generate text\n",
    "# output = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "# # Decode and print the output\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fcf8ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '78.45*t**3 + 70.86*t + 14.0',\n",
       " 'answer': '19.61*t**4 + 35.43*t**2 + 14.0*t',\n",
       " 'input_ids': [3,\n",
       "  3940,\n",
       "  5,\n",
       "  2128,\n",
       "  1935,\n",
       "  17,\n",
       "  19844,\n",
       "  519,\n",
       "  1768,\n",
       "  2861,\n",
       "  5,\n",
       "  3840,\n",
       "  1935,\n",
       "  17,\n",
       "  1768,\n",
       "  209,\n",
       "  15021,\n",
       "  1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [9997,\n",
       "  4241,\n",
       "  1935,\n",
       "  17,\n",
       "  19844,\n",
       "  591,\n",
       "  1768,\n",
       "  3097,\n",
       "  5,\n",
       "  4906,\n",
       "  1935,\n",
       "  17,\n",
       "  19844,\n",
       "  357,\n",
       "  1768,\n",
       "  209,\n",
       "  15021,\n",
       "  1935,\n",
       "  17,\n",
       "  1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3291a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<lambdifygenerated-44890>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.25*t**4 + 1.74*t**3 + 0.68*t**t\n",
      "<lambdifygenerated-44974>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 2.76*t**4 + 0.3*t**3 + 1.56*t**t\n",
      "<lambdifygenerated-45024>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.6*t**5 + 0.75*t**4 + 5.61*t**t\n",
      "<lambdifygenerated-45058>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.8*t**5 + 0.25*t**4 + 0.36*t**t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot sympify 21.78*t**3 - 121.33*t**2 +\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<lambdifygenerated-45120>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.2*t**5 + 1.19*t**3 + 2.27*t**t\n",
      "<lambdifygenerated-45124>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.0*t**4 + 1.08*t**3 + 4.78*t**t\n",
      "<lambdifygenerated-45210>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.6*t**5 + 0.5*t**4 + 0.8*t**t\n",
      "<lambdifygenerated-45240>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.5*t**4 + 2.93*t**3 + 2.35*t**t\n",
      "<lambdifygenerated-45246>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.25*t**4 + 1.77*t**3 + 1.55*t**t\n",
      "<lambdifygenerated-45646>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.0*t**4 + 1.39*t**3 + 0.61*t**t\n",
      "<lambdifygenerated-45704>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.4*t**5 + 1.0*t**4 + 1.0*t**t\n",
      "<lambdifygenerated-45708>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.92*t**6 + 0.92*t**4 + 4.81*t**t\n",
      "<lambdifygenerated-45720>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 4.37*t**5 + 3.02*t**4 + 10.55*t**t\n",
      "<lambdifygenerated-45794>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.4*t**5 + 2.27*t**3 + 3.61*t**t\n",
      "<lambdifygenerated-45848>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.71*t**5 + 2.22*t**3 + 9.44*t**t\n",
      "<lambdifygenerated-45860>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.0*t**5 - 2.0*t**4 + 0.8*t**t\n",
      "<lambdifygenerated-46026>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.75*t**4 + 1.42*t**3 + 0.76*t**t\n",
      "<lambdifygenerated-46088>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.46*t**5 + 1.45*t**3 + 0.89*t**t\n",
      "<lambdifygenerated-46098>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.56*t**4 + 1.01*t**3 + 0.91*t**t\n",
      "<lambdifygenerated-46230>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.8*t**5 + 0.25*t**4 + 0.29*t**t\n",
      "<lambdifygenerated-46270>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.8*t**5 + 2.19*t**3 + 1.77*t**t\n",
      "<lambdifygenerated-46372>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 3.64*t**5 + 0.25*t**4 + 1.36*t**t\n",
      "<lambdifygenerated-46416>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 3.0*t**4 + 0.19*t**3 + 2.92*t**t\n",
      "<lambdifygenerated-46462>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.6*t**5 + 0.25*t**4 + 0.33*t**t\n",
      "<lambdifygenerated-46552>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.8*t**5 + 3.25*t**4 + 3.67*t**t\n",
      "<lambdifygenerated-46564>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.4*t**5 + 7.84*t**3 + 1.11*t**t\n",
      "<lambdifygenerated-46578>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.11*t**5 + 0.29*t**4 + 3.0*t**t\n",
      "<lambdifygenerated-46580>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.8*t**5 + 0.25*t**4 + 1.65*t**t\n",
      "<lambdifygenerated-46640>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 2.75*t**4 + 1.0*t**3 + 1.0*t**t\n",
      "<lambdifygenerated-46758>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 2.0*t**5 - 1.25*t**4 + 1.6*t**t\n",
      "<lambdifygenerated-46762>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 3.2*t**5 + 0.5*t**4 + 3.89*t**t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_processed 953\n",
      "num_equal 645\n",
      "num_zero_diff 645\n",
      "num_within_allowed 813\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from utils.utils import *\n",
    "\n",
    "MIN_ALLOWED_DIFF = 0.011\n",
    "\n",
    "test_ds = tokenized_dataset['test']\n",
    "\n",
    "num_processed = 0\n",
    "num_equal = 0\n",
    "num_zero_diff = 0\n",
    "num_within_allowed = 0\n",
    "t = sp.Symbol('t')\n",
    "\n",
    "verbose = False\n",
    "\n",
    "for i in range(min(len(test_ds), 1000)):\n",
    "    if verbose:\n",
    "        print(\"Case\", i, test_ds[i]['question'])\n",
    "    question = test_ds[i]['question']\n",
    "    if 'repeat' in question:\n",
    "        question = question[0:question.find('repeat')]\n",
    "    original = sp.sympify(question)\n",
    "    integral = round_all_floats(sp.integrate(original), 2)\n",
    "    if verbose: display(integral)\n",
    "    try:\n",
    "        pred = generate_integral(test_ds[i]['question'])\n",
    "        generated = round_all_floats(sp.sympify(pred), 2)\n",
    "        if verbose: display(generated)\n",
    "    except:\n",
    "        print(\"Cannot sympify\", pred)\n",
    "        continue\n",
    "    avg_diff = get_avg_diff(integral, generated, t)\n",
    "    if verbose: print(\"avg_diff\", avg_diff)\n",
    "    num_processed += 1\n",
    "    if avg_diff <= MIN_ALLOWED_DIFF:\n",
    "        num_within_allowed += 1\n",
    "    if avg_diff <= 0.0000001:\n",
    "        num_zero_diff += 1\n",
    "    diff_expr = sp.simplify(integral-generated)\n",
    "    if verbose: display(diff_expr)\n",
    "    if diff_expr == 0:\n",
    "        if verbose: print(\"Equal\")\n",
    "        num_equal += 1\n",
    "        \n",
    "print(\"num_processed\", num_processed)\n",
    "print(\"num_equal\", num_equal)\n",
    "print(\"num_zero_diff\", num_zero_diff)\n",
    "print(\"num_within_allowed\", num_within_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4982312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovering checkpoint 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/3 : < :, Epoch 4/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mcwave/anaconda3/envs/symbolic/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "<lambdifygenerated-48356>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.87*t**4 + 1.03*t**3 + 1.52*t**t\n",
      "<lambdifygenerated-48396>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.0*t**5 + 0.96*t**3 + 13.31*t**t\n",
      "<lambdifygenerated-48420>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.2*t**5 + 0.75*t**4 + 2.42*t**t\n",
      "<lambdifygenerated-48482>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 2.25*t**4 + 0.33*t**3 + 1.39*t**t\n",
      "<lambdifygenerated-48638>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.36*t**6 + 2.22*t**4 + 3.89*t**t\n",
      "<lambdifygenerated-48850>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.4*t**5 + 3.0*t**3 + 6.66*t**t\n",
      "<lambdifygenerated-48858>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.6*t**5 + 0.25*t**4 + 0.41*t**t\n",
      "<lambdifygenerated-49046>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 3.4*t**5 + 4.57*t**4 + 1.31*t**t\n",
      "<lambdifygenerated-49048>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.4*t**5 + 0.48*t**3 + 0.68*t**t\n",
      "<lambdifygenerated-49178>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.8*t**5 + 0.75*t**4 + 5.08*t**t\n",
      "<lambdifygenerated-49180>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.15*t**4 + 3.0*t**3 + 7.81*t**t\n",
      "<lambdifygenerated-49286>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.5*t**4 + 0.86*t**3 + 6.83*t**t\n",
      "<lambdifygenerated-49292>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.06*t**4 + 0.88*t**3 + 1.32*t**t\n",
      "<lambdifygenerated-49472>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.72*t**4 + 1.0*t**3 + 0.36*t**t\n",
      "<lambdifygenerated-49474>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.89*t**5 + 0.44*t**4 + 3.41*t**t\n",
      "<lambdifygenerated-49546>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.5*t**4 + 3.08*t**3 + 2.92*t**t\n",
      "<lambdifygenerated-49594>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.25*t**4 + 2.05*t**3 + 0.72*t**t\n",
      "<lambdifygenerated-49646>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.14*t**7 + 0.01*t**4 + 0.02*t**t\n",
      "<lambdifygenerated-49672>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.83*t**6 + 0.2*t**5 + 2.56*t**t\n",
      "<lambdifygenerated-49778>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.6*t**5 + 0.25*t**4 + 0.33*t**t\n",
      "<lambdifygenerated-49850>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.75*t**4 + 1.41*t**3 + 2.11*t**t\n",
      "<lambdifygenerated-49902>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.25*t**4 + 0.82*t**3 + 2.64*t**t\n",
      "<lambdifygenerated-49960>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 2.75*t**4 + 1.8*t**3 + 1.0*t**t\n",
      "<lambdifygenerated-50098>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.67*t**6 + 1.08*t**3 + 0.02*t**t\n",
      "<lambdifygenerated-50142>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 1.01*t**5 + 2.45*t**4 + 2.43*t**t\n",
      "<lambdifygenerated-50154>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.6*t**5 + 0.57*t**4 + 1.28*t**t\n",
      "<lambdifygenerated-50200>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.5*t**4 + 3.21*t**3 + 0.51*t**t\n",
      "<lambdifygenerated-50274>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.04*t**5 + 5.78*t**4 + 4.51*t**t\n",
      "<lambdifygenerated-50328>:2: RuntimeWarning: invalid value encountered in power\n",
      "  return 0.4*t**5 + 3.9*t**3 + 0.55*t**t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_processed 1000\n",
      "num_equal 669\n",
      "num_zero_diff 669\n",
      "num_within_allowed 875\n",
      "avg. time = 0.1477824866771698\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from utils.utils import *\n",
    "import time\n",
    "\n",
    "MIN_ALLOWED_DIFF = 0.011\n",
    "\n",
    "test_ds = train_ds['test']\n",
    "\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"datasets/integrate_flant5_20240101\",\n",
    "   evaluation_strategy=\"steps\",\n",
    "   learning_rate=2e-7,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   save_steps=SAVE_STEPS,\n",
    "   eval_steps=SAVE_STEPS,\n",
    "   logging_steps=SAVE_STEPS,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=0.001,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")\n",
    "for step in range(12000, 13000, 1000):\n",
    "    print(\"Recovering checkpoint\", step)\n",
    "    cp_path = \"datasets/integrate_flant5_large_20240101/checkpoint-\" + str(step)\n",
    "    trainer.train(cp_path)\n",
    "    #\n",
    "    num_processed = 0\n",
    "    num_equal = 0\n",
    "    num_zero_diff = 0\n",
    "    num_within_allowed = 0\n",
    "    total_time = 0.0\n",
    "    t = sp.Symbol('t')\n",
    "    #\n",
    "    verbose = False\n",
    "    #\n",
    "    for i in range(min(len(test_ds), 1000)):\n",
    "        if i * 100 == 0:\n",
    "            print(i, \"rows processed\")\n",
    "        if verbose:\n",
    "            print(\"Case\", i, test_ds[i]['question'])\n",
    "        question = test_ds[i]['question']\n",
    "        if 'repeat' in question:\n",
    "            question = question[0:question.find('repeat')]\n",
    "        original = sp.sympify(question)\n",
    "        integral = round_all_floats(sp.integrate(original), 2)\n",
    "        if verbose: display(integral)\n",
    "        try:\n",
    "            t1 = time.time()\n",
    "            pred = generate_integral(question)\n",
    "            generated = round_all_floats(sp.sympify(pred), 2)\n",
    "            t2 = time.time()\n",
    "            total_time += (t2-t1)\n",
    "            if verbose: display(generated)\n",
    "            avg_diff = get_avg_diff(integral, generated, t)\n",
    "            if verbose: print(\"avg_diff\", avg_diff)\n",
    "            num_processed += 1\n",
    "            if avg_diff <= MIN_ALLOWED_DIFF:\n",
    "                num_within_allowed += 1\n",
    "            if avg_diff <= 0.0000001:\n",
    "                num_zero_diff += 1\n",
    "            diff_expr = sp.simplify(integral-generated)\n",
    "            if verbose: display(diff_expr)\n",
    "            if diff_expr == 0:\n",
    "                if verbose: print(\"Equal\")\n",
    "                num_equal += 1\n",
    "        except:\n",
    "            print(\"Cannot process\", question)\n",
    "            continue\n",
    "        \n",
    "    #\n",
    "    print(\"num_processed\", num_processed)\n",
    "    print(\"num_equal\", num_equal)\n",
    "    print(\"num_zero_diff\", num_zero_diff)\n",
    "    print(\"num_within_allowed\", num_within_allowed)\n",
    "    print(\"avg. time =\", total_time/num_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:symbolic]",
   "language": "python",
   "name": "conda-env-symbolic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
